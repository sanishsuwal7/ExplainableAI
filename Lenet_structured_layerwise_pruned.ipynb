{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e955b499-d0d7-4620-8253-08a026795785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea03925-7440-4c0a-84fa-c4bb25e6e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7021bbc0-cf85-4f0b-acb8-e3b1b11ef228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root=\"./datasets\", train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdea28bd-9584-42c2-afb1-686bdcc890e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1307]), tensor([0.3081]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "imgs = torch.stack([img for img, _ in train_val_dataset], dim =0)\n",
    "mean = imgs.view(1, -1).mean(dim =1)\n",
    "std = imgs.view(1, -1).std(dim =1)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4542af07-1954-48ee-818f-9993d76f7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=mnist_transforms)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f652e213-d1b8-4f38-b89c-0d1326bb8fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 6000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.9 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd6ff040-4ebb-4db0-8ab8-dfdc3b12c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 188, 313)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Let's see no of batches that we have now with the current batch-size\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9270fe04-967c-47e1-98b6-3d40561c98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5V1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d32c3791-2671-4a62-a5f5-a19faf793a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "model_lenet5v1 = LeNet5V1()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_lenet5v1.parameters(), lr = 0.01)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ed1f5c-a733-44e3-b86b-cc252693096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # device-agnostic setup\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# accuracy = accuracy.to(device)\n",
    "# model_lenet5v1 = model_lenet5v1.to(device)\n",
    "\n",
    "# EPOCHS = 12\n",
    "\n",
    "# for epoch in tqdm(range(EPOCHS)):\n",
    "#     # Training loop\n",
    "#     train_loss, train_acc = 0.0, 0.0\n",
    "#     for X, y in train_dataloader:\n",
    "#         X, y = X.to(device), y.to(device)\n",
    "        \n",
    "#         model_lenet5v1.train()\n",
    "        \n",
    "#         y_pred = model_lenet5v1(X)\n",
    "        \n",
    "#         loss = loss_fn(y_pred, y)\n",
    "#         train_loss += loss.item()\n",
    "        \n",
    "#         acc = accuracy(y_pred, y)\n",
    "#         train_acc += acc\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     train_loss /= len(train_dataloader)\n",
    "#     train_acc /= len(train_dataloader)\n",
    "        \n",
    "#     # Validation loop\n",
    "#     val_loss, val_acc = 0.0, 0.0\n",
    "#     model_lenet5v1.eval()\n",
    "#     with torch.inference_mode():\n",
    "#         for X, y in val_dataloader:\n",
    "#             X, y = X.to(device), y.to(device)\n",
    "            \n",
    "#             y_pred = model_lenet5v1(X)\n",
    "            \n",
    "#             loss = loss_fn(y_pred, y)\n",
    "#             val_loss += loss.item()\n",
    "            \n",
    "#             acc = accuracy(y_pred, y)\n",
    "#             val_acc += acc\n",
    "            \n",
    "#         val_loss /= len(val_dataloader)\n",
    "#         val_acc /= len(val_dataloader)\n",
    "    \n",
    "#     print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37658970-2e82-405f-8699-4ba457d464a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    total_params = 0\n",
    "    for layer_names, param in model.named_parameters():\n",
    "        total_params += torch.count_nonzero(param.data)\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7184f902-044e-4cb8-a5b3-4f1a484941b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned LeNet-5 model has 1199881 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(model_lenet5v1)\n",
    "print(f\"Unpruned LeNet-5 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec4a58de-1685-43b4-b644-91ba3db59438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: conv1.weight & param.shape = torch.Size([32, 1, 3, 3])\n",
      "layer.name: conv1.bias & param.shape = torch.Size([32])\n",
      "layer.name: conv2.weight & param.shape = torch.Size([64, 32, 3, 3])\n",
      "layer.name: conv2.bias & param.shape = torch.Size([64])\n",
      "layer.name: fc1.weight & param.shape = torch.Size([128, 9216])\n",
      "layer.name: fc1.bias & param.shape = torch.Size([128])\n",
      "layer.name: fc2.weight & param.shape = torch.Size([10, 128])\n",
      "layer.name: fc2.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in model_lenet5v1.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e89617fc-d894-43e1-a737-29cecf25b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([32, 1, 3, 3])\n",
      "conv1.bias torch.Size([32])\n",
      "conv2.weight torch.Size([64, 32, 3, 3])\n",
      "conv2.bias torch.Size([64])\n",
      "fc1.weight torch.Size([128, 9216])\n",
      "fc1.bias torch.Size([128])\n",
      "fc2.weight torch.Size([10, 128])\n",
      "fc2.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer_name in model_lenet5v1.state_dict().keys():\n",
    "    print(layer_name, model_lenet5v1.state_dict()[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "367d8c12-508c-467f-a544-49ac4bf7821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lenet5v1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a5380d8-3a4b-4bbc-8328-3a710134c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(model):\n",
    "    conv1_sparsity = (torch.sum(model.conv1.weight == 0) / model.conv1.weight.nelement()) * 100\n",
    "    conv2_sparsity = (torch.sum(model.conv2.weight == 0) / model.conv2.weight.nelement()) * 100\n",
    "    fc1_sparsity = (torch.sum(model.fc1.weight == 0) / model.fc1.weight.nelement()) * 100\n",
    "    op_sparsity = (torch.sum(model.fc2.weight == 0) / model.fc2.weight.nelement()) * 100\n",
    "\n",
    "    num = torch.sum(model.conv1.weight == 0) + torch.sum(model.conv2.weight == 0) + torch.sum(model.fc1.weight == 0) + torch.sum(model.fc2.weight == 0)\n",
    "    denom = model.conv1.weight.nelement() + model.conv2.weight.nelement() + model.fc1.weight.nelement() + model.fc2.weight.nelement()\n",
    "\n",
    "    global_sparsity = num/denom * 100\n",
    "\n",
    "    return global_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbeb79bc-735b-4371-b53a-f91a14ac2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5 global sparsity = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"LeNet-5 global sparsity = {compute_sparsity(model_lenet5v1):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0518e81c-0c47-49be-8719-2e396696fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iterative Global pruning round = 1\n",
      "LeNet-5 global sparsity = 10.14%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.64788| Train acc:  0.79617| Val loss:  0.23024| Val acc:  0.93068\n",
      "Epoch: 1| Train loss:  0.42809| Train acc:  0.86752| Val loss:  0.21763| Val acc:  0.93251\n",
      "Epoch: 2| Train loss:  0.40774| Train acc:  0.87554| Val loss:  0.22863| Val acc:  0.94182\n",
      "Epoch: 3| Train loss:  0.38819| Train acc:  0.88328| Val loss:  0.19034| Val acc:  0.94249\n",
      "Epoch: 4| Train loss:  0.37631| Train acc:  0.88585| Val loss:  0.17694| Val acc:  0.94914\n",
      "Epoch: 5| Train loss:  0.38099| Train acc:  0.88603| Val loss:  0.17760| Val acc:  0.94232\n",
      "Epoch: 6| Train loss:  0.36187| Train acc:  0.89242| Val loss:  0.19060| Val acc:  0.94332\n",
      "Epoch: 7| Train loss:  0.35817| Train acc:  0.89261| Val loss:  0.17934| Val acc:  0.94764\n",
      "Epoch: 8| Train loss:  0.35484| Train acc:  0.89436| Val loss:  0.17289| Val acc:  0.94747\n",
      "Epoch: 9| Train loss:  0.35069| Train acc:  0.89333| Val loss:  0.17896| Val acc:  0.94864\n",
      "Epoch: 10| Train loss:  0.35397| Train acc:  0.89362| Val loss:  0.16781| Val acc:  0.95113\n",
      "Epoch: 11| Train loss:  0.35004| Train acc:  0.89514| Val loss:  0.15486| Val acc:  0.95263\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 2\n",
      "LeNet-5 global sparsity = 19.52%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.49162| Train acc:  0.80467| Val loss:  0.30439| Val acc:  0.85206\n",
      "Epoch: 1| Train loss:  0.47657| Train acc:  0.80896| Val loss:  0.29889| Val acc:  0.85821\n",
      "Epoch: 2| Train loss:  0.47048| Train acc:  0.81015| Val loss:  0.30590| Val acc:  0.85206\n",
      "Epoch: 3| Train loss:  0.47287| Train acc:  0.81013| Val loss:  0.30953| Val acc:  0.84907\n",
      "Epoch: 4| Train loss:  0.46700| Train acc:  0.81096| Val loss:  0.32011| Val acc:  0.85455\n",
      "Epoch: 5| Train loss:  0.46274| Train acc:  0.81087| Val loss:  0.29813| Val acc:  0.85306\n",
      "Epoch: 6| Train loss:  0.47014| Train acc:  0.81159| Val loss:  0.31606| Val acc:  0.84724\n",
      "Epoch: 7| Train loss:  0.47956| Train acc:  0.80658| Val loss:  0.30998| Val acc:  0.84791\n",
      "Epoch: 8| Train loss:  0.47877| Train acc:  0.80963| Val loss:  0.29591| Val acc:  0.85472\n",
      "Epoch: 9| Train loss:  0.47644| Train acc:  0.80648| Val loss:  0.30533| Val acc:  0.85156\n",
      "Epoch: 10| Train loss:  0.46895| Train acc:  0.80980| Val loss:  0.30051| Val acc:  0.85455\n",
      "Epoch: 11| Train loss:  0.46812| Train acc:  0.81089| Val loss:  0.36167| Val acc:  0.83943\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy = accuracy.to(device)\n",
    "model_lenet5v1 = model_lenet5v1.to(device)\n",
    "\n",
    "EPOCHS = 12\n",
    "\n",
    "for iter_prune_round in range(2):\n",
    "    print(f\"\\n\\nIterative Global pruning round = {iter_prune_round + 1}\")\n",
    "    \n",
    "    # Prune layer-wise in a structured manner-\n",
    "    prune.ln_structured(model_lenet5v1.conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.fc1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.fc2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "\n",
    "    # Print current global sparsity level-\n",
    "    print(f\"LeNet-5 global sparsity = {compute_sparsity(model_lenet5v1):.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Fine-training loop-\n",
    "    print(\"\\nFine-tuning pruned model to recover model's performance\\n\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            model_lenet5v1.train()\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            train_acc += acc\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "            \n",
    "        # Validation loop\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        model_lenet5v1.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X, y in val_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                y_pred = model_lenet5v1(X)\n",
    "                \n",
    "                loss = loss_fn(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                acc = accuracy(y_pred, y)\n",
    "                val_acc += acc\n",
    "                \n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc /= len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d691c0d6-54b7-42b3-b715-5be196b1695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'conv1.bias': tensor([-0.1765, -0.2138, -0.2956, -0.4034, -0.3447, -0.4388, -0.3617, -0.3391,\n",
      "        -0.2558, -0.1817, -0.0759, -0.2134, -0.3188, -0.2356, -0.2306, -0.4459,\n",
      "        -0.1513, -0.1414, -0.2197, -0.1255, -0.4387, -0.1542, -0.6356, -0.0012,\n",
      "        -0.3279, -0.2402, -0.0015, -0.1514, -0.4706, -0.3682, -0.3107, -0.3700],\n",
      "       device='cuda:0'), 'conv1.weight_orig': tensor([[[[-0.0638,  0.2512,  0.0241],\n",
      "          [-0.0077, -0.0086,  0.2295],\n",
      "          [-0.2666, -0.2892,  0.1790]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1059,  0.2335, -0.4714],\n",
      "          [-0.2065,  0.3846, -0.2992],\n",
      "          [-0.3419,  0.1817,  0.0993]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2011, -0.2696,  0.3057],\n",
      "          [-0.0553, -0.4592,  0.2178],\n",
      "          [ 0.2844, -0.1168, -0.0028]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1878, -0.0242, -0.0665],\n",
      "          [ 0.1380, -0.1186, -0.0161],\n",
      "          [-0.2211, -0.2948,  0.3633]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2028, -0.1597,  0.3027],\n",
      "          [-0.3570,  0.1262, -0.3636],\n",
      "          [ 0.1878, -0.1347,  0.1959]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2723, -0.3339,  0.0398],\n",
      "          [-0.2085,  0.2645, -0.2841],\n",
      "          [ 0.0854, -0.0081,  0.1267]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2972,  0.3695, -0.2202],\n",
      "          [ 0.1416, -0.3130, -0.4380],\n",
      "          [-0.4338, -0.3364,  0.1982]]],\n",
      "\n",
      "\n",
      "        [[[-0.1175,  0.1008, -0.3593],\n",
      "          [-0.2889,  0.1178, -0.0081],\n",
      "          [-0.1139,  0.0533,  0.1979]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0887, -0.1513,  0.1320],\n",
      "          [ 0.1035,  0.1751,  0.2419],\n",
      "          [ 0.0907, -0.1570, -0.3132]]],\n",
      "\n",
      "\n",
      "        [[[-0.1070, -0.3244, -0.0702],\n",
      "          [-0.2633,  0.1442,  0.0818],\n",
      "          [-0.2758, -0.0133,  0.1666]]],\n",
      "\n",
      "\n",
      "        [[[-0.3352,  0.0798,  0.2033],\n",
      "          [-0.0785,  0.2590, -0.0267],\n",
      "          [ 0.2999, -0.0543, -0.2831]]],\n",
      "\n",
      "\n",
      "        [[[-0.2483, -0.2424,  0.2871],\n",
      "          [-0.0430,  0.0328, -0.1829],\n",
      "          [-0.1945, -0.2409, -0.1629]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2747, -0.0990,  0.2216],\n",
      "          [-0.1103,  0.1242, -0.0944],\n",
      "          [ 0.2391, -0.0470, -0.4509]]],\n",
      "\n",
      "\n",
      "        [[[-0.2450, -0.2172,  0.2804],\n",
      "          [-0.3154,  0.1791,  0.1273],\n",
      "          [-0.3002, -0.2434, -0.2587]]],\n",
      "\n",
      "\n",
      "        [[[-0.3272, -0.1691, -0.3271],\n",
      "          [-0.1600,  0.0355, -0.0058],\n",
      "          [ 0.4480, -0.0473, -0.5105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1497, -0.0997, -0.2965],\n",
      "          [ 0.2354, -0.1957,  0.1338],\n",
      "          [ 0.2047, -0.0596,  0.1785]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0585, -0.1454,  0.2159],\n",
      "          [-0.0653,  0.3215, -0.1001],\n",
      "          [ 0.0858, -0.1632,  0.0474]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1697,  0.1013,  0.2333],\n",
      "          [-0.3212,  0.3594, -0.1526],\n",
      "          [-0.2571, -0.5250, -0.5253]]],\n",
      "\n",
      "\n",
      "        [[[-0.0975, -0.1251,  0.1082],\n",
      "          [-0.1143,  0.1860,  0.0308],\n",
      "          [-0.2098,  0.2040, -0.2446]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2566,  0.0379,  0.0753],\n",
      "          [-0.2465,  0.0082,  0.1535],\n",
      "          [-0.1020,  0.0182, -0.2200]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0614, -0.1020,  0.0734],\n",
      "          [ 0.1418, -0.0432, -0.1456],\n",
      "          [-0.0253, -0.2231,  0.1065]]],\n",
      "\n",
      "\n",
      "        [[[-0.2832, -0.4430, -0.0077],\n",
      "          [ 0.3385,  0.1057, -0.0019],\n",
      "          [-0.1000,  0.2566, -0.0984]]],\n",
      "\n",
      "\n",
      "        [[[-0.1389,  0.1036,  0.3441],\n",
      "          [-0.1019, -0.0498, -0.2048],\n",
      "          [ 0.3342,  0.1092, -0.0343]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0755,  0.1945,  0.2410],\n",
      "          [ 0.2259,  0.0722,  0.0580],\n",
      "          [-0.1817,  0.1171, -0.0211]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2693, -0.1985, -0.4364],\n",
      "          [ 0.2056,  0.0524, -0.3260],\n",
      "          [ 0.1977, -0.2647, -0.0206]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2426,  0.1613, -0.1261],\n",
      "          [-0.2044, -0.0153,  0.1341],\n",
      "          [-0.2622,  0.1012, -0.1228]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2527,  0.1426, -0.0869],\n",
      "          [ 0.0442, -0.1962, -0.2214],\n",
      "          [-0.0816, -0.1298, -0.0781]]],\n",
      "\n",
      "\n",
      "        [[[-0.3370, -0.0785, -0.0745],\n",
      "          [-0.0987,  0.0930, -0.1803],\n",
      "          [ 0.0865, -0.0176,  0.1751]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2481,  0.2639, -0.2687],\n",
      "          [-0.2470, -0.2075, -0.0670],\n",
      "          [ 0.0546,  0.2058,  0.1739]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2437, -0.2509, -0.2867],\n",
      "          [-0.0932, -0.1544, -0.0064],\n",
      "          [-0.1221, -0.1174, -0.1705]]],\n",
      "\n",
      "\n",
      "        [[[-0.2809,  0.1749,  0.1415],\n",
      "          [-0.0358, -0.1064,  0.1174],\n",
      "          [ 0.1717,  0.0891, -0.0214]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0625,  0.0136,  0.2021],\n",
      "          [-0.1253,  0.1503, -0.0226],\n",
      "          [ 0.0283, -0.1339, -0.1571]]]], device='cuda:0'), 'conv1.weight_mask': tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [0., 0., 0.],\n",
      "          [0., 0., 0.]]]], device='cuda:0'), 'conv2.bias': tensor([-0.0769, -0.1086,  0.0260,  0.0797, -0.0771, -0.0328,  0.0360,  0.0021,\n",
      "        -0.1343, -0.1562, -0.0720, -0.1002, -0.1149, -0.0074, -0.0199, -0.0606,\n",
      "        -0.1156, -0.0879, -0.0454, -0.0020, -0.0240, -0.0212, -0.0719, -0.0485,\n",
      "        -0.1413, -0.0712, -0.0135,  0.0073, -0.0910, -0.0227, -0.0509, -0.0715,\n",
      "        -0.0401,  0.0286, -0.0222,  0.1010, -0.1228, -0.0247, -0.0074,  0.0177,\n",
      "         0.0285, -0.1245,  0.0421, -0.0400, -0.0524, -0.0588, -0.0958, -0.0383,\n",
      "        -0.0723, -0.1882, -0.0945, -0.0005, -0.0247, -0.0397, -0.0118, -0.0044,\n",
      "        -0.0039, -0.0024,  0.0100, -0.0007, -0.0069, -0.0649,  0.1857, -0.0149],\n",
      "       device='cuda:0'), 'conv2.weight_orig': tensor([[[[-0.1906, -0.1533, -0.0951],\n",
      "          [-0.0589,  0.0578, -0.0830],\n",
      "          [-0.2119, -0.3850, -0.2891]],\n",
      "\n",
      "         [[ 0.2327,  0.1029, -0.3270],\n",
      "          [ 0.1657,  0.0881, -0.0944],\n",
      "          [-0.2249, -0.1648,  0.1401]],\n",
      "\n",
      "         [[-0.4925, -0.2670, -0.0036],\n",
      "          [-0.3212, -0.0316,  0.0064],\n",
      "          [-0.3741, -0.0848, -0.0602]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2197, -0.1359, -0.0288],\n",
      "          [ 0.0961, -0.2409, -0.4023],\n",
      "          [-0.1687, -0.1524,  0.1167]],\n",
      "\n",
      "         [[-0.2809, -0.1148, -0.0347],\n",
      "          [-0.1515, -0.0781, -0.0525],\n",
      "          [ 0.0323,  0.0670,  0.0403]],\n",
      "\n",
      "         [[-0.3461, -0.2919, -0.2053],\n",
      "          [-0.4022, -0.2170,  0.0323],\n",
      "          [-0.1507, -0.0796, -0.1265]]],\n",
      "\n",
      "\n",
      "        [[[-0.2354, -0.1670, -0.2370],\n",
      "          [-0.0020,  0.1191,  0.0741],\n",
      "          [-0.1790,  0.0250,  0.2156]],\n",
      "\n",
      "         [[-0.0638,  0.0126, -0.1515],\n",
      "          [ 0.0704,  0.1667,  0.1203],\n",
      "          [-0.1441,  0.1390,  0.1670]],\n",
      "\n",
      "         [[-0.0991,  0.1269, -0.0384],\n",
      "          [-0.2869,  0.1391,  0.1227],\n",
      "          [ 0.0683,  0.1204,  0.1154]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4110, -0.3175, -0.3005],\n",
      "          [-0.3107, -0.0930,  0.1009],\n",
      "          [-0.1319,  0.1067, -0.1547]],\n",
      "\n",
      "         [[-0.1588, -0.2207, -0.2780],\n",
      "          [-0.2087, -0.2067, -0.2424],\n",
      "          [-0.2468,  0.0451,  0.1649]],\n",
      "\n",
      "         [[-0.0916, -0.2271, -0.3003],\n",
      "          [-0.0077, -0.0642, -0.0816],\n",
      "          [-0.1265, -0.0452, -0.0350]]],\n",
      "\n",
      "\n",
      "        [[[-0.2975, -0.2991, -0.3588],\n",
      "          [-0.0724, -0.1082, -0.3947],\n",
      "          [ 0.0766,  0.0147, -0.1035]],\n",
      "\n",
      "         [[-0.1209, -0.2077, -0.2250],\n",
      "          [-0.0447, -0.1069,  0.0045],\n",
      "          [ 0.2896,  0.0657,  0.3172]],\n",
      "\n",
      "         [[-0.3048, -0.1865, -0.0654],\n",
      "          [-0.0773,  0.0881,  0.0558],\n",
      "          [-0.1435,  0.2472,  0.2346]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0182, -0.1073,  0.0572],\n",
      "          [-0.1079, -0.1793,  0.2022],\n",
      "          [-0.1160, -0.1025, -0.2591]],\n",
      "\n",
      "         [[-0.1398, -0.1118, -0.2238],\n",
      "          [ 0.0997, -0.0322, -0.1230],\n",
      "          [ 0.1038, -0.0252, -0.0281]],\n",
      "\n",
      "         [[-0.4421, -0.4755, -0.3644],\n",
      "          [-0.0011, -0.0635, -0.2285],\n",
      "          [ 0.0249,  0.0912, -0.0377]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0731, -0.0435,  0.0524],\n",
      "          [ 0.0597, -0.0408,  0.0456],\n",
      "          [-0.4094, -0.2378, -0.2025]],\n",
      "\n",
      "         [[ 0.4046, -0.0211,  0.0671],\n",
      "          [-0.2546,  0.0289, -0.2625],\n",
      "          [-0.1283, -0.1519, -0.2975]],\n",
      "\n",
      "         [[ 0.0378,  0.0134, -0.2977],\n",
      "          [-0.0802,  0.1057, -0.1546],\n",
      "          [ 0.0234, -0.1059, -0.1363]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4134,  0.0155,  0.0977],\n",
      "          [-0.3347,  0.0107,  0.0306],\n",
      "          [ 0.0100, -0.1677, -0.1802]],\n",
      "\n",
      "         [[-0.2341,  0.1371,  0.0743],\n",
      "          [-0.0530, -0.1486, -0.0317],\n",
      "          [-0.1211, -0.2466, -0.2971]],\n",
      "\n",
      "         [[-0.0237,  0.0005, -0.0998],\n",
      "          [-0.1140,  0.0861,  0.0662],\n",
      "          [-0.3219, -0.2262, -0.1593]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2757,  0.0040,  0.1700],\n",
      "          [-0.0172,  0.0179,  0.0186],\n",
      "          [ 0.0010, -0.1918,  0.1039]],\n",
      "\n",
      "         [[-0.1076,  0.0915,  0.0810],\n",
      "          [-0.2831, -0.2723, -0.1071],\n",
      "          [-0.2745,  0.0131, -0.0047]],\n",
      "\n",
      "         [[-0.2086, -0.0758,  0.0269],\n",
      "          [-0.2845, -0.1425,  0.0870],\n",
      "          [-0.2425, -0.2485, -0.0715]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1544, -0.0484, -0.1703],\n",
      "          [ 0.0019,  0.0237, -0.0030],\n",
      "          [-0.0576,  0.0064,  0.0035]],\n",
      "\n",
      "         [[ 0.1742,  0.0401,  0.1265],\n",
      "          [ 0.1749,  0.0466,  0.1058],\n",
      "          [ 0.0823, -0.0430,  0.0791]],\n",
      "\n",
      "         [[ 0.1049,  0.0057,  0.0576],\n",
      "          [ 0.0230, -0.0393,  0.0554],\n",
      "          [ 0.0059,  0.0260,  0.1188]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1743, -0.2084, -0.2207],\n",
      "          [-0.0303, -0.2812, -0.0953],\n",
      "          [-0.0813, -0.1663, -0.1210]],\n",
      "\n",
      "         [[ 0.0454,  0.0944,  0.0495],\n",
      "          [-0.0056, -0.1742,  0.2043],\n",
      "          [-0.0883, -0.3098,  0.1942]],\n",
      "\n",
      "         [[ 0.0222,  0.1228,  0.2232],\n",
      "          [ 0.2670,  0.1233, -0.0089],\n",
      "          [ 0.1528,  0.1580,  0.1203]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0350,  0.2959,  0.1685],\n",
      "          [ 0.0310,  0.0794, -0.2212],\n",
      "          [-0.1016, -0.3516, -0.2852]],\n",
      "\n",
      "         [[ 0.1118, -0.0051, -0.1163],\n",
      "          [ 0.0844, -0.0148, -0.0893],\n",
      "          [-0.0125, -0.0847,  0.0611]],\n",
      "\n",
      "         [[ 0.0052, -0.0250, -0.0071],\n",
      "          [ 0.0811, -0.0441, -0.1546],\n",
      "          [ 0.0826, -0.0636, -0.0876]]]], device='cuda:0'), 'conv2.weight_mask': tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], device='cuda:0'), 'fc1.bias': tensor([-9.8275e-03, -2.7449e-02, -2.2524e-01, -8.7675e-05, -5.0702e-03,\n",
      "        -5.0295e-01,  7.4369e-04,  4.9270e-03, -1.0540e-01, -2.8753e-01,\n",
      "        -2.3741e-01, -3.0373e-02, -2.0355e-01, -6.9236e-02,  3.0657e-03,\n",
      "        -9.4347e-04, -2.6702e-01, -3.2549e-02, -3.4891e-01, -4.8061e-02,\n",
      "        -1.6608e-01, -2.7188e-01, -1.4774e-03, -1.1758e-01, -3.1294e-01,\n",
      "        -1.8980e-01, -7.7304e-03, -2.9448e-01, -2.2543e-01, -3.5603e-01,\n",
      "        -2.5187e-01, -4.4706e-03, -2.2612e-01, -1.4099e-03, -2.4308e-01,\n",
      "        -2.7241e-02, -2.8571e-01, -1.9092e-03, -2.7166e-01, -8.7902e-03,\n",
      "        -2.3567e-01, -1.8656e-01, -9.9301e-02, -7.7151e-02, -1.2677e-02,\n",
      "        -4.6552e-01, -1.0820e-02, -3.5672e-01, -5.2098e-01, -2.8780e-01,\n",
      "        -2.2686e-01, -3.1176e-02, -1.1395e-01, -4.4446e-01, -3.4346e-01,\n",
      "        -1.6667e-03, -4.6568e-02,  2.1911e-03, -1.3759e-02, -1.5344e-01,\n",
      "        -2.4787e-01, -6.9087e-02, -7.7299e-03, -2.9840e-01, -8.4545e-03,\n",
      "        -9.1050e-02, -1.6832e-01, -3.5614e-03, -2.7606e-01, -2.8159e-01,\n",
      "        -3.2871e-01, -2.9770e-02, -1.4445e-01, -8.6171e-02, -1.0645e-02,\n",
      "        -3.9324e-01, -1.8229e-01, -3.3284e-01, -9.2317e-03, -1.8910e-01,\n",
      "        -2.3548e-02, -2.7965e-01, -3.9161e-01, -6.8272e-03, -1.2602e-01,\n",
      "        -8.9487e-02, -1.3012e-01, -2.5039e-01, -4.1503e-01, -1.8306e-01,\n",
      "        -2.8020e-01, -2.2783e-02, -2.1544e-01, -1.9302e-03, -3.2533e-01,\n",
      "        -3.0801e-01, -3.6649e-01, -1.3883e-01, -2.7989e-03, -3.5358e-01,\n",
      "        -2.2962e-01, -1.1315e-01, -8.4894e-03, -1.8010e-01, -3.7647e-01,\n",
      "        -7.3920e-03, -4.0409e-01, -3.8875e-01, -1.8522e-01, -6.3454e-02,\n",
      "        -2.3931e-01, -4.9512e-03, -1.8636e-03, -1.1216e-02, -3.3850e-01,\n",
      "        -2.6208e-03, -2.2740e-01, -6.4562e-03, -4.0286e-03, -7.4887e-03,\n",
      "        -1.2531e-01, -2.1466e-01, -7.4679e-03, -1.9396e-01, -3.6437e-02,\n",
      "        -4.2645e-01, -7.4235e-02, -4.9490e-01], device='cuda:0'), 'fc1.weight_orig': tensor([[-0.0077, -0.0034,  0.0075,  ...,  0.0011,  0.0081,  0.0055],\n",
      "        [ 0.0836, -0.0173,  0.0472,  ...,  0.1750,  0.1935,  0.0611],\n",
      "        [ 0.1574,  0.2792,  0.3374,  ..., -0.0118,  0.0250, -0.0300],\n",
      "        ...,\n",
      "        [ 0.0148, -0.0198,  0.1093,  ...,  0.0523,  0.1288,  0.0109],\n",
      "        [ 0.0128,  0.0240, -0.0224,  ...,  0.0707,  0.0324, -0.0698],\n",
      "        [ 0.0665,  0.0342, -0.0142,  ..., -0.1369,  0.0010,  0.0410]],\n",
      "       device='cuda:0'), 'fc1.weight_mask': tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0'), 'fc2.bias': tensor([ 0.3546,  0.3476, -0.1360, -0.1425, -0.1420,  0.2192, -0.2548, -0.0702,\n",
      "         0.1971,  0.0847], device='cuda:0'), 'fc2.weight_orig': tensor([[-0.0473, -0.4231, -0.2318,  ...,  0.0952,  0.1153,  0.1647],\n",
      "        [-0.0522,  0.0104, -0.1974,  ..., -0.3026,  0.1058, -0.2680],\n",
      "        [ 0.0769, -0.2775,  0.1593,  ...,  0.0896, -0.1734, -0.0989],\n",
      "        ...,\n",
      "        [-0.0167,  0.0280, -0.0605,  ..., -0.0646, -0.0719,  0.0805],\n",
      "        [-0.0265, -0.4397, -0.2784,  ...,  0.1030, -0.1011,  0.1825],\n",
      "        [ 0.0812, -0.3110, -0.2935,  ...,  0.0895, -0.3119,  0.1244]],\n",
      "       device='cuda:0'), 'fc2.weight_mask': tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0')})\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"lenet5_v1_mnist_prune.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "\n",
    "# Saving the model\n",
    "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_lenet5v1.state_dict(), f=MODEL_SAVE_PATH)\n",
    "\n",
    "# Loading the saved model\n",
    "model_lenet5_v1_mnist_prune_loaded = LeNet5V1()\n",
    "\n",
    "\n",
    "\n",
    "# Load the cleaned state dict into your model\n",
    "model_lenet5_v1_mnist_prune_loaded.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940be3f4-9b70-47ca-bcc1-88f042f7327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = 0, 0\n",
    "\n",
    "model_lenet5v1.to(device)\n",
    "\n",
    "model_lenet5v1.eval()\n",
    "with torch.inference_mode():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model_lenet5v1(X)\n",
    "        \n",
    "        test_loss += loss_fn(y_pred, y)\n",
    "        test_acc += accuracy(y_pred, y)\n",
    "        \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "print(f\"Test loss: {test_loss: .5f}| Test acc: {test_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b331ed0a-d197-497f-ac6d-c57b0c0d7969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
