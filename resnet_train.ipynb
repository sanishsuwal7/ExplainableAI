{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190d7068-3845-4785-858e-f9cb7d3f96af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ranger import Ranger\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import torch.optim as optim\n",
    "from cleverhans.torch.attacks.projected_gradient_descent import (projected_gradient_descent)\n",
    "\n",
    "import quantus\n",
    "import captum\n",
    "from captum.attr import Saliency, IntegratedGradients, NoiseTunnel\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaebe740-bbc8-4a75-9964-bfa48b4b826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d31c189-46fe-4af8-92c7-ae6f11fa528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "train_path = 'datasets/imagenette2/train'\n",
    "val_path =  'datasets/imagenette2/val'\n",
    "train_dataloader = torch.utils.data.DataLoader(datasets.ImageFolder(train_path, \n",
    "                                                                   transform = transforms.Compose([\n",
    "                                                                        transforms.RandomResizedCrop(224),\n",
    "                                                                        transforms.RandomHorizontalFlip(),\n",
    "                                                                        transforms.ToTensor(),\n",
    "                                                                        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                             std=[0.229, 0.224, 0.225])\n",
    "                                                                   ])), batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(datasets.ImageFolder(val_path,\n",
    "                                                               transform=transforms.Compose([\n",
    "                                                                   transforms.ToTensor(),\n",
    "                                                                   transforms.Resize([224, 224]),\n",
    "                                                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                        std=[0.229, 0.224, 0.225])\n",
    "                                                               ])),batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cec1059-6a54-46f5-aa5f-fb0af4239db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('tench', 'springer', 'casette_player', 'chain_saw','church', 'French_horn', 'garbage_truck', 'gas_pump', 'golf_ball', 'parachute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9155890-3118-4bf3-8ed5-0f211e749036",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run resnet_features.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b440d8d2-f0e5-4fe1-b66d-b285142fba1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "model_normal = resnet18_features(pretrained=False, filter='None', filter_layer=0)\n",
    "learning_rate = 8e-03\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model_normal.parameters(), lr = learning_rate, eps = 1e-06)\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a24ddf-a6a0-4346-8b65-9d54e27ab231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data, device):\n",
    "    model.eval()\n",
    "    logits = torch.Tensor().to(device)\n",
    "    targets = torch.LongTensor().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in data:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            logits = torch.cat([logits, model(x_batch)])\n",
    "            targets = torch.cat([targets, y_batch])\n",
    "    \n",
    "    return torch.nn.functional.softmax(logits, dim=1), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "507abe16-c344-4459-9a4b-7c7379dc3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_normal(model, epochs):\n",
    "    # best_acc = 0\n",
    "\n",
    "    # for epoch in range(epochs):\n",
    "    #     model.train()\n",
    "    #     for _, (x_batch, y_batch) in enumerate(train_dataloader):\n",
    "    #         x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "    #         logits = model(x_batch)\n",
    "    #         loss = criterion(logits, y_batch)\n",
    "    #         optimizer.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #     model.eval()\n",
    "    #     correct = 0\n",
    "    #     total = 0\n",
    "    #     for _, (x_batch, y_batch) in enumerate(test_dataloader):\n",
    "    #         x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "    #         logits = model(x_batch)\n",
    "    #         pred = torch.argmax(logits, dim=1)\n",
    "    #         correct += torch.sum((y_batch == pred).long()).cpu().numpy()\n",
    "    #         total += x_batch.size(0)\n",
    "    #     epoch_acc = correct / total\n",
    "        print(f\"Epoch {epoch + 1}/ {epochs} - test accuracy:{(100 * epoch_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate model!\n",
    "        if epochs%10==0:\n",
    "            predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "            test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - test accuracy: {(100 * test_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59fe186c-77f3-4f88-8833-c55498bed3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 - test accuracy: 70.42% and CE loss 1.15\n",
      "Epoch 2/150 - test accuracy: 67.80% and CE loss 0.96\n",
      "Epoch 3/150 - test accuracy: 71.03% and CE loss 1.14\n",
      "Epoch 4/150 - test accuracy: 74.04% and CE loss 0.86\n",
      "Epoch 5/150 - test accuracy: 69.89% and CE loss 1.18\n",
      "Epoch 6/150 - test accuracy: 74.73% and CE loss 0.85\n",
      "Epoch 7/150 - test accuracy: 78.50% and CE loss 0.72\n",
      "Epoch 8/150 - test accuracy: 72.94% and CE loss 0.94\n",
      "Epoch 9/150 - test accuracy: 75.11% and CE loss 0.87\n",
      "Epoch 10/150 - test accuracy: 78.19% and CE loss 0.90\n",
      "Epoch 11/150 - test accuracy: 76.79% and CE loss 0.95\n",
      "Epoch 12/150 - test accuracy: 79.87% and CE loss 0.71\n",
      "Epoch 13/150 - test accuracy: 81.25% and CE loss 0.60\n",
      "Epoch 14/150 - test accuracy: 81.25% and CE loss 0.73\n",
      "Epoch 15/150 - test accuracy: 79.03% and CE loss 0.59\n",
      "Epoch 16/150 - test accuracy: 81.83% and CE loss 0.51\n",
      "Epoch 17/150 - test accuracy: 80.41% and CE loss 0.59\n",
      "Epoch 18/150 - test accuracy: 81.89% and CE loss 0.48\n",
      "Epoch 19/150 - test accuracy: 82.93% and CE loss 0.74\n",
      "Epoch 20/150 - test accuracy: 79.41% and CE loss 0.62\n",
      "Epoch 21/150 - test accuracy: 83.39% and CE loss 0.44\n",
      "Epoch 22/150 - test accuracy: 83.46% and CE loss 0.45\n",
      "Epoch 23/150 - test accuracy: 83.31% and CE loss 0.56\n",
      "Epoch 24/150 - test accuracy: 84.41% and CE loss 0.34\n",
      "Epoch 25/150 - test accuracy: 83.92% and CE loss 0.57\n",
      "Epoch 26/150 - test accuracy: 82.98% and CE loss 0.44\n",
      "Epoch 27/150 - test accuracy: 84.48% and CE loss 0.43\n",
      "Epoch 28/150 - test accuracy: 85.73% and CE loss 0.54\n",
      "Epoch 29/150 - test accuracy: 83.69% and CE loss 0.55\n",
      "Epoch 30/150 - test accuracy: 84.82% and CE loss 0.27\n",
      "Epoch 31/150 - test accuracy: 86.83% and CE loss 0.27\n",
      "Epoch 32/150 - test accuracy: 84.36% and CE loss 0.33\n",
      "Epoch 33/150 - test accuracy: 84.76% and CE loss 0.34\n",
      "Epoch 34/150 - test accuracy: 86.50% and CE loss 0.60\n",
      "Epoch 35/150 - test accuracy: 83.57% and CE loss 0.44\n",
      "Epoch 36/150 - test accuracy: 85.30% and CE loss 0.43\n",
      "Epoch 37/150 - test accuracy: 85.61% and CE loss 0.57\n",
      "Epoch 38/150 - test accuracy: 84.54% and CE loss 0.69\n",
      "Epoch 39/150 - test accuracy: 85.91% and CE loss 0.42\n",
      "Epoch 40/150 - test accuracy: 85.58% and CE loss 0.52\n",
      "Epoch 41/150 - test accuracy: 86.52% and CE loss 0.45\n",
      "Epoch 42/150 - test accuracy: 84.48% and CE loss 0.34\n",
      "Epoch 43/150 - test accuracy: 86.34% and CE loss 0.47\n",
      "Epoch 44/150 - test accuracy: 84.28% and CE loss 0.55\n",
      "Epoch 45/150 - test accuracy: 86.19% and CE loss 0.39\n",
      "Epoch 46/150 - test accuracy: 86.80% and CE loss 0.34\n",
      "Epoch 47/150 - test accuracy: 86.29% and CE loss 0.35\n",
      "Epoch 48/150 - test accuracy: 86.04% and CE loss 0.36\n",
      "Epoch 49/150 - test accuracy: 86.57% and CE loss 0.22\n",
      "Epoch 50/150 - test accuracy: 85.86% and CE loss 0.37\n",
      "Epoch 51/150 - test accuracy: 86.37% and CE loss 0.42\n",
      "Epoch 52/150 - test accuracy: 87.18% and CE loss 0.37\n",
      "Epoch 53/150 - test accuracy: 86.32% and CE loss 0.36\n",
      "Epoch 54/150 - test accuracy: 85.71% and CE loss 0.24\n",
      "Epoch 55/150 - test accuracy: 87.82% and CE loss 0.31\n",
      "Epoch 56/150 - test accuracy: 86.45% and CE loss 0.21\n",
      "Epoch 57/150 - test accuracy: 86.62% and CE loss 0.13\n",
      "Epoch 58/150 - test accuracy: 86.39% and CE loss 0.43\n",
      "Epoch 59/150 - test accuracy: 87.39% and CE loss 0.32\n",
      "Epoch 60/150 - test accuracy: 85.22% and CE loss 0.52\n",
      "Epoch 61/150 - test accuracy: 87.08% and CE loss 0.35\n",
      "Epoch 62/150 - test accuracy: 87.01% and CE loss 0.28\n",
      "Epoch 63/150 - test accuracy: 86.19% and CE loss 0.39\n",
      "Epoch 64/150 - test accuracy: 87.80% and CE loss 0.27\n",
      "Epoch 65/150 - test accuracy: 86.93% and CE loss 0.34\n",
      "Epoch 66/150 - test accuracy: 87.24% and CE loss 0.25\n",
      "Epoch 67/150 - test accuracy: 87.46% and CE loss 0.14\n",
      "Epoch 68/150 - test accuracy: 86.65% and CE loss 0.38\n",
      "Epoch 69/150 - test accuracy: 87.34% and CE loss 0.28\n",
      "Epoch 70/150 - test accuracy: 87.54% and CE loss 0.44\n",
      "Epoch 71/150 - test accuracy: 87.52% and CE loss 0.16\n",
      "Epoch 72/150 - test accuracy: 87.41% and CE loss 0.27\n",
      "Epoch 73/150 - test accuracy: 88.03% and CE loss 0.34\n",
      "Epoch 74/150 - test accuracy: 87.59% and CE loss 0.22\n",
      "Epoch 75/150 - test accuracy: 88.20% and CE loss 0.26\n",
      "Epoch 76/150 - test accuracy: 87.72% and CE loss 0.30\n",
      "Epoch 77/150 - test accuracy: 86.80% and CE loss 0.38\n",
      "Epoch 78/150 - test accuracy: 88.20% and CE loss 0.21\n",
      "Epoch 79/150 - test accuracy: 87.31% and CE loss 0.16\n",
      "Epoch 80/150 - test accuracy: 87.72% and CE loss 0.32\n",
      "Epoch 81/150 - test accuracy: 87.72% and CE loss 0.42\n",
      "Epoch 82/150 - test accuracy: 88.23% and CE loss 0.44\n",
      "Epoch 83/150 - test accuracy: 86.83% and CE loss 0.25\n",
      "Epoch 84/150 - test accuracy: 88.71% and CE loss 0.13\n",
      "Epoch 85/150 - test accuracy: 89.58% and CE loss 0.35\n",
      "Epoch 86/150 - test accuracy: 88.33% and CE loss 0.22\n",
      "Epoch 87/150 - test accuracy: 87.39% and CE loss 0.06\n",
      "Epoch 88/150 - test accuracy: 88.41% and CE loss 0.35\n",
      "Epoch 89/150 - test accuracy: 87.62% and CE loss 0.10\n",
      "Epoch 90/150 - test accuracy: 88.38% and CE loss 0.31\n",
      "Epoch 91/150 - test accuracy: 87.92% and CE loss 0.14\n",
      "Epoch 92/150 - test accuracy: 86.83% and CE loss 0.19\n",
      "Epoch 93/150 - test accuracy: 87.06% and CE loss 0.23\n",
      "Epoch 94/150 - test accuracy: 88.41% and CE loss 0.39\n",
      "Epoch 95/150 - test accuracy: 86.24% and CE loss 0.18\n",
      "Epoch 96/150 - test accuracy: 87.08% and CE loss 0.37\n",
      "Epoch 97/150 - test accuracy: 88.13% and CE loss 0.15\n",
      "Epoch 98/150 - test accuracy: 87.41% and CE loss 0.18\n",
      "Epoch 99/150 - test accuracy: 87.75% and CE loss 0.16\n",
      "Epoch 100/150 - test accuracy: 87.72% and CE loss 0.41\n",
      "Epoch 101/150 - test accuracy: 88.20% and CE loss 0.12\n",
      "Epoch 102/150 - test accuracy: 87.72% and CE loss 0.25\n",
      "Epoch 103/150 - test accuracy: 89.10% and CE loss 0.12\n",
      "Epoch 104/150 - test accuracy: 88.13% and CE loss 0.28\n",
      "Epoch 105/150 - test accuracy: 88.43% and CE loss 0.27\n",
      "Epoch 106/150 - test accuracy: 87.90% and CE loss 0.21\n",
      "Epoch 107/150 - test accuracy: 87.92% and CE loss 0.20\n",
      "Epoch 108/150 - test accuracy: 88.18% and CE loss 0.34\n",
      "Epoch 109/150 - test accuracy: 86.24% and CE loss 0.43\n",
      "Epoch 110/150 - test accuracy: 86.90% and CE loss 0.26\n",
      "Epoch 111/150 - test accuracy: 88.38% and CE loss 0.18\n",
      "Epoch 112/150 - test accuracy: 87.64% and CE loss 0.31\n",
      "Epoch 113/150 - test accuracy: 87.16% and CE loss 0.23\n",
      "Epoch 114/150 - test accuracy: 87.49% and CE loss 0.60\n",
      "Epoch 115/150 - test accuracy: 88.89% and CE loss 0.14\n",
      "Epoch 116/150 - test accuracy: 87.64% and CE loss 0.26\n",
      "Epoch 117/150 - test accuracy: 87.44% and CE loss 0.19\n",
      "Epoch 118/150 - test accuracy: 88.51% and CE loss 0.32\n",
      "Epoch 119/150 - test accuracy: 87.97% and CE loss 0.21\n",
      "Epoch 120/150 - test accuracy: 87.34% and CE loss 0.15\n",
      "Epoch 121/150 - test accuracy: 87.29% and CE loss 0.27\n",
      "Epoch 122/150 - test accuracy: 88.18% and CE loss 0.13\n",
      "Epoch 123/150 - test accuracy: 86.88% and CE loss 0.09\n",
      "Epoch 124/150 - test accuracy: 87.34% and CE loss 0.24\n",
      "Epoch 125/150 - test accuracy: 88.18% and CE loss 0.16\n",
      "Epoch 126/150 - test accuracy: 87.24% and CE loss 0.22\n",
      "Epoch 127/150 - test accuracy: 88.08% and CE loss 0.30\n",
      "Epoch 128/150 - test accuracy: 88.18% and CE loss 0.13\n",
      "Epoch 129/150 - test accuracy: 88.61% and CE loss 0.29\n",
      "Epoch 130/150 - test accuracy: 88.87% and CE loss 0.12\n",
      "Epoch 131/150 - test accuracy: 87.85% and CE loss 0.29\n",
      "Epoch 132/150 - test accuracy: 87.46% and CE loss 0.16\n",
      "Epoch 133/150 - test accuracy: 88.28% and CE loss 0.18\n",
      "Epoch 134/150 - test accuracy: 87.54% and CE loss 0.18\n",
      "Epoch 135/150 - test accuracy: 88.69% and CE loss 0.16\n",
      "Epoch 136/150 - test accuracy: 88.05% and CE loss 0.13\n",
      "Epoch 137/150 - test accuracy: 88.89% and CE loss 0.38\n",
      "Epoch 138/150 - test accuracy: 88.64% and CE loss 0.12\n",
      "Epoch 139/150 - test accuracy: 88.13% and CE loss 0.28\n",
      "Epoch 140/150 - test accuracy: 87.01% and CE loss 0.22\n",
      "Epoch 141/150 - test accuracy: 86.68% and CE loss 0.31\n",
      "Epoch 142/150 - test accuracy: 88.61% and CE loss 0.26\n",
      "Epoch 143/150 - test accuracy: 88.33% and CE loss 0.14\n",
      "Epoch 144/150 - test accuracy: 88.84% and CE loss 0.12\n",
      "Epoch 145/150 - test accuracy: 89.17% and CE loss 0.23\n",
      "Epoch 146/150 - test accuracy: 87.41% and CE loss 0.20\n",
      "Epoch 147/150 - test accuracy: 87.34% and CE loss 0.20\n",
      "Epoch 148/150 - test accuracy: 87.77% and CE loss 0.38\n",
      "Epoch 149/150 - test accuracy: 87.41% and CE loss 0.47\n",
      "Epoch 150/150 - test accuracy: 87.67% and CE loss 0.18\n"
     ]
    }
   ],
   "source": [
    "model_normal = train_normal(model = model_normal.to(device), epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f617f6bf-c78b-4740-9b6a-4502bbb0ecbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test accuracy: 87.67%\n"
     ]
    }
   ],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model_normal.to(device)\n",
    "model_normal.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model_normal, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Model test accuracy: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d1d718e5-8134-43a1-9220-dabd7a3adf7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model: models\\resnet_imagenette.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"resnet_imagenette.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_normal.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7104303-560e-4394-bd24-131daa20818a",
   "metadata": {},
   "source": [
    "# Adversarial Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29c7568b-ab47-4abc-a44c-48000f348b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0/255\n",
    "k = 20\n",
    "alpha = 0.00784\n",
    "\n",
    "class LinfPGDAttack(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def perturb(self, x_natural, y):\n",
    "        x = x_natural.detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-epsilon, epsilon)\n",
    "        for i in range(k):\n",
    "            x.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits = self.model(x)\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "            grad = torch.autograd.grad(loss, [x])[0]\n",
    "            x = x.detach() + alpha * torch.sign(grad.detach())\n",
    "            x = torch.min(torch.max(x, x_natural - epsilon), x_natural + epsilon)\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "        return x\n",
    "    \n",
    "def attack(x, y, model, adversary):\n",
    "    model_copied = copy.deepcopy(model)\n",
    "    model_copied.eval()\n",
    "    adversary.model = model_copied\n",
    "    adv = adversary.perturb(x, y)\n",
    "    return adv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "855888da-4ac4-4c84-a27b-e6c3423a53b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "model = resnet18_features(pretrained=False, filter='None', filter_layer=0)\n",
    "adversary = LinfPGDAttack(model)\n",
    "learning_rate = 8e-03\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(), lr = learning_rate, eps = 1e-06)\n",
    "# lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9049ca1-fd7a-4584-b918-adda2d6a02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adv(model, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            adv = adversary.perturb(x_batch, y_batch)\n",
    "            logits = model(adv)\n",
    "            # x_batch = projected_gradient_descent(model, x_batch, eps, eps/10, 40, np.inf)\n",
    "            # logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate model!\n",
    "        if epochs%10==0:\n",
    "            predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "            test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - test accuracy: {(100 * test_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c53a91-51a8-48eb-8b9c-97e57a33da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adversarial = train_adv(model=model.to(device),\n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd149a5-b399-42b9-a184-dd519627db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model_adversarial.to(device)\n",
    "model_adversarial.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model_adversarial, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Model test accuracy: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666465ad-e8dd-46c5-92ae-32d75d7c1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"resnet_imagenette_adv.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_adversarial.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78812c-77b0-4a0b-a048-215f468ccb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98682c-7840-4059-9862-e54048f09036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06349a79-add0-4fab-8ae4-8a1c8ec2db6d",
   "metadata": {},
   "source": [
    "# L1 Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "5084a422-ec5f-4e60-8945-4c7e935661fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "model = resnet18_features(pretrained=False, filter='None', filter_layer=0)\n",
    "learning_rate = 1e-03\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(), lr = learning_rate, eps = 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e1262f71-cca5-4cf2-af95-68bd0c88f7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    total_params = 0\n",
    "    for layer_names, param in model.named_parameters():\n",
    "        total_params += torch.count_nonzero(param.data)\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4fdc8f87-2cba-42fb-97c0-39602d9dbd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned RESNET-18 model has 11176840 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(model)\n",
    "print(f\"Unpruned RESNET-18 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "2181affd-5400-4917-84c4-b6c67b31bfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: conv1.weight & param.shape = torch.Size([64, 3, 7, 7])\n",
      "layer.name: bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.conv1.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.0.bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.conv2.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.0.bn2.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.bn2.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.conv1.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.1.bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.conv2.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.1.bn2.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.bn2.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer2.0.conv1.weight & param.shape = torch.Size([128, 64, 3, 3])\n",
      "layer.name: layer2.0.bn1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.bn1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.conv2.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.0.bn2.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.bn2.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.downsample.0.weight & param.shape = torch.Size([128, 64, 1, 1])\n",
      "layer.name: layer2.0.downsample.1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.downsample.1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.conv1.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.1.bn1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.bn1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.conv2.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.1.bn2.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.bn2.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer3.0.conv1.weight & param.shape = torch.Size([256, 128, 3, 3])\n",
      "layer.name: layer3.0.bn1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.bn1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.conv2.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.0.bn2.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.bn2.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.downsample.0.weight & param.shape = torch.Size([256, 128, 1, 1])\n",
      "layer.name: layer3.0.downsample.1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.downsample.1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.conv1.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.1.bn1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.bn1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.conv2.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.1.bn2.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.bn2.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer4.0.conv1.weight & param.shape = torch.Size([512, 256, 3, 3])\n",
      "layer.name: layer4.0.bn1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.bn1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.conv2.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.0.bn2.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.bn2.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.downsample.0.weight & param.shape = torch.Size([512, 256, 1, 1])\n",
      "layer.name: layer4.0.downsample.1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.downsample.1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.conv1.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.1.bn1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.bn1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.conv2.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.1.bn2.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.bn2.bias & param.shape = torch.Size([512])\n",
      "layer.name: fc.weight & param.shape = torch.Size([10, 512])\n",
      "layer.name: fc.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in model.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "64baf07d-bbf2-437d-8549-f575b56c25ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([64, 3, 7, 7])\n",
      "bn1.weight torch.Size([64])\n",
      "bn1.bias torch.Size([64])\n",
      "bn1.running_mean torch.Size([64])\n",
      "bn1.running_var torch.Size([64])\n",
      "bn1.num_batches_tracked torch.Size([])\n",
      "layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn1.weight torch.Size([64])\n",
      "layer1.0.bn1.bias torch.Size([64])\n",
      "layer1.0.bn1.running_mean torch.Size([64])\n",
      "layer1.0.bn1.running_var torch.Size([64])\n",
      "layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.0.bn2.weight torch.Size([64])\n",
      "layer1.0.bn2.bias torch.Size([64])\n",
      "layer1.0.bn2.running_mean torch.Size([64])\n",
      "layer1.0.bn2.running_var torch.Size([64])\n",
      "layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn1.weight torch.Size([64])\n",
      "layer1.1.bn1.bias torch.Size([64])\n",
      "layer1.1.bn1.running_mean torch.Size([64])\n",
      "layer1.1.bn1.running_var torch.Size([64])\n",
      "layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "layer1.1.bn2.weight torch.Size([64])\n",
      "layer1.1.bn2.bias torch.Size([64])\n",
      "layer1.1.bn2.running_mean torch.Size([64])\n",
      "layer1.1.bn2.running_var torch.Size([64])\n",
      "layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "layer2.0.bn1.weight torch.Size([128])\n",
      "layer2.0.bn1.bias torch.Size([128])\n",
      "layer2.0.bn1.running_mean torch.Size([128])\n",
      "layer2.0.bn1.running_var torch.Size([128])\n",
      "layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.0.bn2.weight torch.Size([128])\n",
      "layer2.0.bn2.bias torch.Size([128])\n",
      "layer2.0.bn2.running_mean torch.Size([128])\n",
      "layer2.0.bn2.running_var torch.Size([128])\n",
      "layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "layer2.0.downsample.1.weight torch.Size([128])\n",
      "layer2.0.downsample.1.bias torch.Size([128])\n",
      "layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "layer2.0.downsample.1.running_var torch.Size([128])\n",
      "layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn1.weight torch.Size([128])\n",
      "layer2.1.bn1.bias torch.Size([128])\n",
      "layer2.1.bn1.running_mean torch.Size([128])\n",
      "layer2.1.bn1.running_var torch.Size([128])\n",
      "layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "layer2.1.bn2.weight torch.Size([128])\n",
      "layer2.1.bn2.bias torch.Size([128])\n",
      "layer2.1.bn2.running_mean torch.Size([128])\n",
      "layer2.1.bn2.running_var torch.Size([128])\n",
      "layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "layer3.0.bn1.weight torch.Size([256])\n",
      "layer3.0.bn1.bias torch.Size([256])\n",
      "layer3.0.bn1.running_mean torch.Size([256])\n",
      "layer3.0.bn1.running_var torch.Size([256])\n",
      "layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.0.bn2.weight torch.Size([256])\n",
      "layer3.0.bn2.bias torch.Size([256])\n",
      "layer3.0.bn2.running_mean torch.Size([256])\n",
      "layer3.0.bn2.running_var torch.Size([256])\n",
      "layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "layer3.0.downsample.1.weight torch.Size([256])\n",
      "layer3.0.downsample.1.bias torch.Size([256])\n",
      "layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "layer3.0.downsample.1.running_var torch.Size([256])\n",
      "layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn1.weight torch.Size([256])\n",
      "layer3.1.bn1.bias torch.Size([256])\n",
      "layer3.1.bn1.running_mean torch.Size([256])\n",
      "layer3.1.bn1.running_var torch.Size([256])\n",
      "layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "layer3.1.bn2.weight torch.Size([256])\n",
      "layer3.1.bn2.bias torch.Size([256])\n",
      "layer3.1.bn2.running_mean torch.Size([256])\n",
      "layer3.1.bn2.running_var torch.Size([256])\n",
      "layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "layer4.0.bn1.weight torch.Size([512])\n",
      "layer4.0.bn1.bias torch.Size([512])\n",
      "layer4.0.bn1.running_mean torch.Size([512])\n",
      "layer4.0.bn1.running_var torch.Size([512])\n",
      "layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.0.bn2.weight torch.Size([512])\n",
      "layer4.0.bn2.bias torch.Size([512])\n",
      "layer4.0.bn2.running_mean torch.Size([512])\n",
      "layer4.0.bn2.running_var torch.Size([512])\n",
      "layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "layer4.0.downsample.1.weight torch.Size([512])\n",
      "layer4.0.downsample.1.bias torch.Size([512])\n",
      "layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "layer4.0.downsample.1.running_var torch.Size([512])\n",
      "layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn1.weight torch.Size([512])\n",
      "layer4.1.bn1.bias torch.Size([512])\n",
      "layer4.1.bn1.running_mean torch.Size([512])\n",
      "layer4.1.bn1.running_var torch.Size([512])\n",
      "layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "layer4.1.bn2.weight torch.Size([512])\n",
      "layer4.1.bn2.bias torch.Size([512])\n",
      "layer4.1.bn2.running_mean torch.Size([512])\n",
      "layer4.1.bn2.running_var torch.Size([512])\n",
      "layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "fc.weight torch.Size([10, 512])\n",
      "fc.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer_name in model.state_dict().keys():\n",
    "    print(layer_name, model.state_dict()[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "d847e14d-8199-4803-9310-48b418ae8c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7e98fa0a-5415-46ad-8156-bfb0fd64707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(model):\n",
    "    conv0_sparsity = (torch.sum(model.conv1.weight == 0) / model.conv1.weight.nelement()) * 100\n",
    "    bn0_sparsity = (torch.sum(model.bn1.weight == 0) / model.bn1.weight.nelement()) * 100\n",
    "    \n",
    "    conv1_sparsity = (torch.sum(model.layer1[0].conv1.weight == 0) / model.layer1[0].conv1.weight.nelement()) * 100\n",
    "    conv1_sparsity = (torch.sum(model.layer1[0].bn1.weight == 0) / model.layer1[0].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv2_sparsity = (torch.sum(model.layer1[0].conv2.weight == 0) / model.layer1[0].conv2.weight.nelement()) * 100\n",
    "    conv2_sparsity = (torch.sum(model.layer1[0].bn2.weight == 0) / model.layer1[0].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv3_sparsity = (torch.sum(model.layer1[1].conv1.weight == 0) / model.layer1[1].conv1.weight.nelement()) * 100\n",
    "    conv3_sparsity = (torch.sum(model.layer1[1].bn1.weight == 0) / model.layer1[1].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv4_sparsity = (torch.sum(model.layer1[1].conv2.weight == 0) / model.layer1[1].conv2.weight.nelement()) * 100\n",
    "    conv4_sparsity = (torch.sum(model.layer1[1].bn2.weight == 0) / model.layer1[1].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv5_sparsity = (torch.sum(model.layer2[0].conv1.weight == 0) / model.layer2[0].conv1.weight.nelement()) * 100\n",
    "    conv5_sparsity = (torch.sum(model.layer2[0].bn1.weight == 0) / model.layer2[0].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv6_sparsity = (torch.sum(model.layer2[0].conv2.weight == 0) / model.layer2[0].conv2.weight.nelement()) * 100\n",
    "    conv6_sparsity = (torch.sum(model.layer2[0].bn2.weight == 0) / model.layer2[0].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv7_sparsity = (torch.sum(model.layer2[1].conv1.weight == 0) / model.layer2[1].conv1.weight.nelement()) * 100\n",
    "    conv7_sparsity = (torch.sum(model.layer2[1].bn1.weight == 0) / model.layer2[1].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv8_sparsity = (torch.sum(model.layer2[1].conv2.weight == 0) / model.layer2[1].conv2.weight.nelement()) * 100\n",
    "    conv8_sparsity = (torch.sum(model.layer2[1].bn2.weight == 0) / model.layer2[1].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv9_sparsity = (torch.sum(model.layer3[0].conv1.weight == 0) / model.layer3[0].conv1.weight.nelement()) * 100\n",
    "    conv9_sparsity = (torch.sum(model.layer3[0].bn1.weight == 0) / model.layer3[0].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv10_sparsity = (torch.sum(model.layer3[0].conv2.weight == 0) / model.layer3[0].conv2.weight.nelement()) * 100\n",
    "    conv10_sparsity = (torch.sum(model.layer3[0].bn2.weight == 0) / model.layer3[0].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv11_sparsity = (torch.sum(model.layer3[1].conv1.weight == 0) / model.layer3[1].conv1.weight.nelement()) * 100\n",
    "    conv11_sparsity = (torch.sum(model.layer3[1].bn1.weight == 0) / model.layer3[1].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv12_sparsity = (torch.sum(model.layer3[1].conv2.weight == 0) / model.layer3[1].conv2.weight.nelement()) * 100\n",
    "    conv12_sparsity = (torch.sum(model.layer3[1].bn2.weight == 0) / model.layer3[1].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv13_sparsity = (torch.sum(model.layer4[0].conv1.weight == 0) / model.layer4[0].conv1.weight.nelement()) * 100\n",
    "    conv13_sparsity = (torch.sum(model.layer4[0].bn1.weight == 0) / model.layer4[0].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv14_sparsity = (torch.sum(model.layer4[0].conv2.weight == 0) / model.layer4[0].conv2.weight.nelement()) * 100\n",
    "    conv14_sparsity = (torch.sum(model.layer4[0].bn2.weight == 0) / model.layer4[0].bn2.weight.nelement()) * 100\n",
    "\n",
    "    conv15_sparsity = (torch.sum(model.layer4[1].conv1.weight == 0) / model.layer4[1].conv1.weight.nelement()) * 100\n",
    "    conv15_sparsity = (torch.sum(model.layer4[1].bn1.weight == 0) / model.layer4[1].bn1.weight.nelement()) * 100\n",
    "\n",
    "    conv16_sparsity = (torch.sum(model.layer4[1].conv2.weight == 0) / model.layer4[1].conv2.weight.nelement()) * 100\n",
    "    conv16_sparsity = (torch.sum(model.layer4[1].bn2.weight == 0) / model.layer4[1].bn2.weight.nelement()) * 100\n",
    "    \n",
    "    fc_sparsity = (torch.sum(model.fc.weight == 0) / model.fc.weight.nelement()) * 100\n",
    "\n",
    "    num =  torch.sum(model.conv1.weight == 0) + torch.sum(model.bn1.weight == 0) + torch.sum(model.layer1[0].conv1.weight == 0) + torch.sum(model.layer1[0].bn1.weight == 0) + torch.sum(model.layer1[0].conv2.weight == 0) +  torch.sum(model.layer1[0].bn2.weight == 0) + torch.sum(model.layer1[1].conv1.weight == 0) +  torch.sum(model.layer1[1].bn1.weight == 0) + torch.sum(model.layer1[1].conv2.weight == 0) + torch.sum(model.layer1[1].bn2.weight == 0) +torch.sum(model.layer2[0].conv1.weight == 0) + torch.sum(model.layer2[0].bn1.weight == 0) + torch.sum(model.layer2[0].conv2.weight == 0) +  torch.sum(model.layer2[0].bn2.weight == 0) + torch.sum(model.layer2[1].conv1.weight == 0) + torch.sum(model.layer2[1].bn1.weight == 0) + torch.sum(model.layer2[1].conv2.weight == 0) + torch.sum(model.layer2[1].bn2.weight == 0) + torch.sum(model.layer3[0].conv1.weight == 0) + torch.sum(model.layer3[0].bn1.weight == 0) + torch.sum(model.layer3[0].conv2.weight == 0) +  torch.sum(model.layer3[0].bn2.weight == 0) + torch.sum(model.layer3[1].conv1.weight == 0) +  torch.sum(model.layer3[1].bn1.weight == 0) + torch.sum(model.layer3[1].conv2.weight == 0) + torch.sum(model.layer3[1].bn2.weight == 0) + torch.sum(model.layer4[0].conv1.weight == 0) + torch.sum(model.layer4[0].bn1.weight == 0) + torch.sum(model.layer4[0].conv2.weight == 0) +  torch.sum(model.layer4[0].bn2.weight == 0) + torch.sum(model.layer4[1].conv1.weight == 0) +  torch.sum(model.layer4[1].bn1.weight == 0) + torch.sum(model.layer4[1].conv2.weight == 0) + torch.sum(model.layer4[1].bn2.weight == 0) + torch.sum(model.fc.weight == 0) \n",
    "                                                                                                                                                                                                                                                                                                                                    \n",
    "    denom =  model.conv1.weight.nelement() +  model.bn1.weight.nelement() + model.layer1[0].conv1.weight.nelement() + model.layer1[0].bn1.weight.nelement() + model.layer1[0].conv2.weight.nelement() + model.layer1[0].bn2.weight.nelement() + model.layer1[1].conv1.weight.nelement() +  model.layer1[1].bn1.weight.nelement() + model.layer1[1].conv2.weight.nelement() + model.layer1[1].bn2.weight.nelement() +  model.layer2[0].conv1.weight.nelement() + model.layer2[0].bn1.weight.nelement() + model.layer2[0].conv2.weight.nelement() + model.layer2[0].bn2.weight.nelement() + model.layer2[1].conv1.weight.nelement() +  model.layer2[1].bn1.weight.nelement() + model.layer2[1].conv2.weight.nelement() + model.layer2[1].bn2.weight.nelement() +  model.layer3[0].conv1.weight.nelement() + model.layer3[0].bn1.weight.nelement() + model.layer3[0].conv2.weight.nelement() + model.layer3[0].bn2.weight.nelement() + model.layer3[1].conv1.weight.nelement() +  model.layer3[1].bn1.weight.nelement() + model.layer3[1].conv2.weight.nelement() + model.layer3[1].bn2.weight.nelement() +  model.layer4[0].conv1.weight.nelement() + model.layer4[0].bn1.weight.nelement() + model.layer4[0].conv2.weight.nelement() + model.layer4[0].bn2.weight.nelement() + model.layer4[1].conv1.weight.nelement() +  model.layer4[1].bn1.weight.nelement() + model.layer4[1].conv2.weight.nelement() + model.layer4[1].bn2.weight.nelement() + model.fc.weight.nelement()\n",
    "    global_sparsity = num/denom * 100\n",
    "    return global_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "be104047-bd94-462e-9fa8-a7cb3cf4a3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESNET-18 global sparsity = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"RESNET-18 global sparsity = {compute_sparsity(model):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "fa810cb0-e6c8-4f0d-94a5-8e2d3259941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "for name, module in model.named_modules():\n",
    "    # prune 20% of weights/connections in for all hidden layaers-\n",
    "    if isinstance(module, torch.nn.Conv2d):\n",
    "        prune.l1_unstructured(module = module, name = 'weight', amount = 0.2)\n",
    "    \n",
    "    # prune 10% of weights/connections for output layer-\n",
    "    elif isinstance(module, torch.nn.Linear):\n",
    "        prune.l1_unstructured(module = module, name = 'weight', amount = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "cb0c48e8-e511-4247-a450-889d6d003d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESNET-18 global sparsity = 19.99%\n"
     ]
    }
   ],
   "source": [
    "print(f\"RESNET-18 global sparsity = {compute_sparsity(model):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fdd3c4a0-de44-42eb-9b6e-41ec59272dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_l1_prune(model, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in train_dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate model!\n",
    "        if epochs%10==0:\n",
    "            predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "            test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - test accuracy: {(100 * test_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6270e997-37c6-4aa9-b2ef-6950222807cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - test accuracy: 46.37% and CE loss 1.50\n",
      "Epoch 2/100 - test accuracy: 52.84% and CE loss 1.52\n",
      "Epoch 3/100 - test accuracy: 66.04% and CE loss 1.23\n",
      "Epoch 4/100 - test accuracy: 62.62% and CE loss 1.20\n",
      "Epoch 5/100 - test accuracy: 67.49% and CE loss 0.92\n",
      "Epoch 6/100 - test accuracy: 74.06% and CE loss 1.06\n",
      "Epoch 7/100 - test accuracy: 71.26% and CE loss 0.67\n",
      "Epoch 8/100 - test accuracy: 71.95% and CE loss 0.56\n",
      "Epoch 9/100 - test accuracy: 76.89% and CE loss 0.83\n",
      "Epoch 10/100 - test accuracy: 74.22% and CE loss 0.86\n",
      "Epoch 11/100 - test accuracy: 75.59% and CE loss 0.81\n",
      "Epoch 12/100 - test accuracy: 78.17% and CE loss 0.93\n",
      "Epoch 13/100 - test accuracy: 75.54% and CE loss 0.78\n",
      "Epoch 14/100 - test accuracy: 76.10% and CE loss 0.80\n",
      "Epoch 15/100 - test accuracy: 80.89% and CE loss 0.60\n",
      "Epoch 16/100 - test accuracy: 79.26% and CE loss 0.63\n",
      "Epoch 17/100 - test accuracy: 77.71% and CE loss 0.62\n",
      "Epoch 18/100 - test accuracy: 80.10% and CE loss 1.06\n",
      "Epoch 19/100 - test accuracy: 79.46% and CE loss 0.85\n",
      "Epoch 20/100 - test accuracy: 79.24% and CE loss 0.47\n",
      "Epoch 21/100 - test accuracy: 80.76% and CE loss 0.92\n",
      "Epoch 22/100 - test accuracy: 78.62% and CE loss 0.39\n",
      "Epoch 23/100 - test accuracy: 80.99% and CE loss 0.60\n",
      "Epoch 24/100 - test accuracy: 83.21% and CE loss 0.49\n",
      "Epoch 25/100 - test accuracy: 81.40% and CE loss 0.54\n",
      "Epoch 26/100 - test accuracy: 80.74% and CE loss 0.38\n",
      "Epoch 27/100 - test accuracy: 83.11% and CE loss 0.48\n",
      "Epoch 28/100 - test accuracy: 80.43% and CE loss 0.36\n",
      "Epoch 29/100 - test accuracy: 83.49% and CE loss 0.56\n",
      "Epoch 30/100 - test accuracy: 83.67% and CE loss 0.60\n",
      "Epoch 31/100 - test accuracy: 84.00% and CE loss 0.40\n",
      "Epoch 32/100 - test accuracy: 81.48% and CE loss 0.47\n",
      "Epoch 33/100 - test accuracy: 84.79% and CE loss 0.49\n",
      "Epoch 34/100 - test accuracy: 84.38% and CE loss 0.63\n",
      "Epoch 35/100 - test accuracy: 83.67% and CE loss 0.67\n",
      "Epoch 36/100 - test accuracy: 84.05% and CE loss 0.64\n",
      "Epoch 37/100 - test accuracy: 83.62% and CE loss 0.37\n",
      "Epoch 38/100 - test accuracy: 82.83% and CE loss 0.40\n",
      "Epoch 39/100 - test accuracy: 85.12% and CE loss 0.50\n",
      "Epoch 40/100 - test accuracy: 83.72% and CE loss 0.33\n",
      "Epoch 41/100 - test accuracy: 84.71% and CE loss 0.52\n",
      "Epoch 42/100 - test accuracy: 84.51% and CE loss 0.61\n",
      "Epoch 43/100 - test accuracy: 83.97% and CE loss 0.58\n",
      "Epoch 44/100 - test accuracy: 83.39% and CE loss 0.25\n",
      "Epoch 45/100 - test accuracy: 85.04% and CE loss 0.66\n",
      "Epoch 46/100 - test accuracy: 84.94% and CE loss 0.38\n",
      "Epoch 47/100 - test accuracy: 85.20% and CE loss 0.42\n",
      "Epoch 48/100 - test accuracy: 85.61% and CE loss 0.34\n",
      "Epoch 49/100 - test accuracy: 81.17% and CE loss 0.39\n",
      "Epoch 50/100 - test accuracy: 85.55% and CE loss 0.34\n",
      "Epoch 51/100 - test accuracy: 86.39% and CE loss 0.58\n",
      "Epoch 52/100 - test accuracy: 85.04% and CE loss 0.22\n",
      "Epoch 53/100 - test accuracy: 85.07% and CE loss 0.37\n",
      "Epoch 54/100 - test accuracy: 84.00% and CE loss 0.39\n",
      "Epoch 55/100 - test accuracy: 85.45% and CE loss 0.15\n",
      "Epoch 56/100 - test accuracy: 86.17% and CE loss 0.38\n",
      "Epoch 57/100 - test accuracy: 85.12% and CE loss 0.32\n",
      "Epoch 58/100 - test accuracy: 84.13% and CE loss 0.31\n",
      "Epoch 59/100 - test accuracy: 84.69% and CE loss 0.39\n",
      "Epoch 60/100 - test accuracy: 86.50% and CE loss 0.54\n",
      "Epoch 61/100 - test accuracy: 85.86% and CE loss 0.27\n",
      "Epoch 62/100 - test accuracy: 86.50% and CE loss 0.43\n",
      "Epoch 63/100 - test accuracy: 86.17% and CE loss 0.29\n",
      "Epoch 64/100 - test accuracy: 86.24% and CE loss 0.49\n",
      "Epoch 65/100 - test accuracy: 86.01% and CE loss 0.34\n",
      "Epoch 66/100 - test accuracy: 86.90% and CE loss 0.27\n",
      "Epoch 67/100 - test accuracy: 85.27% and CE loss 0.24\n",
      "Epoch 68/100 - test accuracy: 86.34% and CE loss 0.20\n",
      "Epoch 69/100 - test accuracy: 86.55% and CE loss 0.29\n",
      "Epoch 70/100 - test accuracy: 85.58% and CE loss 0.45\n",
      "Epoch 71/100 - test accuracy: 85.58% and CE loss 0.37\n",
      "Epoch 72/100 - test accuracy: 86.06% and CE loss 0.51\n",
      "Epoch 73/100 - test accuracy: 84.15% and CE loss 0.65\n",
      "Epoch 74/100 - test accuracy: 86.55% and CE loss 0.48\n",
      "Epoch 75/100 - test accuracy: 86.39% and CE loss 0.39\n",
      "Epoch 76/100 - test accuracy: 86.17% and CE loss 0.25\n",
      "Epoch 77/100 - test accuracy: 87.06% and CE loss 0.35\n",
      "Epoch 78/100 - test accuracy: 86.32% and CE loss 0.62\n",
      "Epoch 79/100 - test accuracy: 85.83% and CE loss 0.16\n",
      "Epoch 80/100 - test accuracy: 86.14% and CE loss 0.16\n",
      "Epoch 81/100 - test accuracy: 86.22% and CE loss 0.16\n",
      "Epoch 82/100 - test accuracy: 84.82% and CE loss 0.15\n",
      "Epoch 83/100 - test accuracy: 85.48% and CE loss 0.20\n",
      "Epoch 84/100 - test accuracy: 85.76% and CE loss 0.31\n",
      "Epoch 85/100 - test accuracy: 86.70% and CE loss 0.12\n",
      "Epoch 86/100 - test accuracy: 85.76% and CE loss 0.32\n",
      "Epoch 87/100 - test accuracy: 86.06% and CE loss 0.30\n",
      "Epoch 88/100 - test accuracy: 84.87% and CE loss 0.23\n",
      "Epoch 89/100 - test accuracy: 86.45% and CE loss 0.18\n",
      "Epoch 90/100 - test accuracy: 87.75% and CE loss 0.25\n",
      "Epoch 91/100 - test accuracy: 86.98% and CE loss 0.47\n",
      "Epoch 92/100 - test accuracy: 86.11% and CE loss 0.23\n",
      "Epoch 93/100 - test accuracy: 87.21% and CE loss 0.23\n",
      "Epoch 94/100 - test accuracy: 85.71% and CE loss 0.44\n",
      "Epoch 95/100 - test accuracy: 86.14% and CE loss 0.19\n",
      "Epoch 96/100 - test accuracy: 86.60% and CE loss 0.21\n",
      "Epoch 97/100 - test accuracy: 86.45% and CE loss 0.10\n",
      "Epoch 98/100 - test accuracy: 87.85% and CE loss 0.16\n",
      "Epoch 99/100 - test accuracy: 86.85% and CE loss 0.36\n",
      "Epoch 100/100 - test accuracy: 85.94% and CE loss 0.29\n"
     ]
    }
   ],
   "source": [
    "model_l1_unstructured = train_l1_prune(model = model.to(device), epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ddfff428-87d5-4a7a-a127-beb05652fbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test accuracy: 85.94%\n"
     ]
    }
   ],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model_l1_unstructured.to(device)\n",
    "model_l1_unstructured.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model_l1_unstructured, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Model test accuracy: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cbb59e36-ebc7-49fa-a9ea-c4f376179761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model: models\\resnet_imagenette_l1_unstructured.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"resnet_imagenette_l1_unstructured.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_l1_unstructured.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10b88c8-9776-4562-827c-89299727e6ca",
   "metadata": {},
   "source": [
    "# Global Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "76400dd7-72a0-453a-90d3-ff20f7e90850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "model = resnet18_features(pretrained=False, filter='None', filter_layer=0)\n",
    "learning_rate = 1e-03\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(), lr = learning_rate, eps = 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0d991d47-e9ad-4628-a58f-7aaf22a6838e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned RESNET-18 model has 11176842 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(model)\n",
    "print(f\"Unpruned RESNET-18 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "d42bf80b-6e99-423f-9414-ab43b22118e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: conv1.weight & param.shape = torch.Size([64, 3, 7, 7])\n",
      "layer.name: bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.conv1.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.0.bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.conv2.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.0.bn2.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.bn2.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.conv1.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.1.bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.conv2.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.1.bn2.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.bn2.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer2.0.conv1.weight & param.shape = torch.Size([128, 64, 3, 3])\n",
      "layer.name: layer2.0.bn1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.bn1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.conv2.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.0.bn2.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.bn2.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.downsample.0.weight & param.shape = torch.Size([128, 64, 1, 1])\n",
      "layer.name: layer2.0.downsample.1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.downsample.1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.conv1.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.1.bn1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.bn1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.conv2.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.1.bn2.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.bn2.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer3.0.conv1.weight & param.shape = torch.Size([256, 128, 3, 3])\n",
      "layer.name: layer3.0.bn1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.bn1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.conv2.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.0.bn2.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.bn2.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.downsample.0.weight & param.shape = torch.Size([256, 128, 1, 1])\n",
      "layer.name: layer3.0.downsample.1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.downsample.1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.conv1.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.1.bn1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.bn1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.conv2.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.1.bn2.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.bn2.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer4.0.conv1.weight & param.shape = torch.Size([512, 256, 3, 3])\n",
      "layer.name: layer4.0.bn1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.bn1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.conv2.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.0.bn2.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.bn2.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.downsample.0.weight & param.shape = torch.Size([512, 256, 1, 1])\n",
      "layer.name: layer4.0.downsample.1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.downsample.1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.conv1.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.1.bn1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.bn1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.conv2.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.1.bn2.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.bn2.bias & param.shape = torch.Size([512])\n",
      "layer.name: fc.weight & param.shape = torch.Size([10, 512])\n",
      "layer.name: fc.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in model.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "72b56c2c-e3a2-4498-8545-c8be6b10d27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESNET-18 global sparsity = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"RESNET-18 global sparsity = {compute_sparsity(model):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "df0d529b-ad90-46c8-99c8-0f63de64cf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "e732530c-51d4-479a-bc2e-7912af3fec90",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = (\n",
    "    (model.conv1, 'weight'),\n",
    "    (model.bn1, 'weight'),\n",
    "    (model.layer1[0].conv1, 'weight'),\n",
    "    (model.layer1[0].bn1, 'weight'),\n",
    "    (model.layer1[0].conv2, 'weight'),\n",
    "    (model.layer1[0].bn2, 'weight'),\n",
    "    (model.layer1[1].conv1, 'weight'),\n",
    "    (model.layer1[1].bn1, 'weight'),\n",
    "    (model.layer1[1].conv2, 'weight'),\n",
    "    (model.layer1[1].bn2, 'weight'),\n",
    "    (model.layer2[0].conv1, 'weight'),\n",
    "    (model.layer2[0].bn1, 'weight'),\n",
    "    (model.layer2[0].conv2, 'weight'),\n",
    "    (model.layer2[0].bn2, 'weight'),\n",
    "    (model.layer2[1].conv1, 'weight'),\n",
    "    (model.layer2[1].bn1, 'weight'),\n",
    "    (model.layer2[1].conv2, 'weight'),\n",
    "    (model.layer2[1].bn2, 'weight'),\n",
    "    (model.layer3[0].conv1, 'weight'),\n",
    "    (model.layer3[0].bn1, 'weight'),\n",
    "    (model.layer3[0].conv2, 'weight'),\n",
    "    (model.layer3[0].bn2, 'weight'),\n",
    "    (model.layer3[1].conv1, 'weight'),\n",
    "    (model.layer3[1].bn1, 'weight'),\n",
    "    (model.layer3[1].conv2, 'weight'),\n",
    "    (model.layer3[1].bn2, 'weight'),\n",
    "    (model.layer4[0].conv1, 'weight'),\n",
    "    (model.layer4[0].bn1, 'weight'),\n",
    "    (model.layer4[0].conv2, 'weight'),\n",
    "    (model.layer4[0].bn2, 'weight'),\n",
    "    (model.layer4[1].conv1, 'weight'),\n",
    "    (model.layer4[1].bn1, 'weight'),\n",
    "    (model.layer4[1].conv2, 'weight'),\n",
    "    (model.layer4[1].bn2, 'weight'),\n",
    "    (model.fc, 'weight')\n",
    ")\n",
    "\n",
    "prune_rates_global = [0.2, 0.3, 0.4, 0.5, 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "a95ef648-562e-4a8a-a7e8-7da1175e0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_global_pruned(model, epochs):\n",
    "    for iter_prune_round in range(1):\n",
    "        print(f\"\\n\\nIterative Global pruning round = {iter_prune_round + 1}\")\n",
    "        \n",
    "        # Prune layer-wise in a structured manner-\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method = prune.L1Unstructured,\n",
    "            amount = prune_rates_global[iter_prune_round]\n",
    "            \n",
    "        )\n",
    "    \n",
    "        # Print current global sparsity level-\n",
    "        print(f\" RESNET-18 global sparsity = {compute_sparsity(model):.2f}%\")\n",
    "        \n",
    "        \n",
    "        # Fine-training loop-\n",
    "        print(\"\\nFine-tuning pruned model to recover model's performance\\n\")\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for x_batch, y_batch in train_dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(x_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "            # Evaluate model!\n",
    "            if epochs%10==0:\n",
    "                predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "                test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - test accuracy: {(100 * test_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "acc89f38-ab39-4cd4-9a33-416d5917d524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iterative Global pruning round = 1\n",
      " RESNET-18 global sparsity = 20.00%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch 1/100 - test accuracy: 53.38% and CE loss 1.48\n",
      "Epoch 2/100 - test accuracy: 60.41% and CE loss 1.33\n",
      "Epoch 3/100 - test accuracy: 65.78% and CE loss 1.31\n",
      "Epoch 4/100 - test accuracy: 66.14% and CE loss 1.33\n",
      "Epoch 5/100 - test accuracy: 68.99% and CE loss 0.92\n",
      "Epoch 6/100 - test accuracy: 73.40% and CE loss 0.68\n",
      "Epoch 7/100 - test accuracy: 71.67% and CE loss 1.02\n",
      "Epoch 8/100 - test accuracy: 72.51% and CE loss 1.00\n",
      "Epoch 9/100 - test accuracy: 77.15% and CE loss 0.98\n",
      "Epoch 10/100 - test accuracy: 68.00% and CE loss 0.85\n",
      "Epoch 11/100 - test accuracy: 76.54% and CE loss 0.66\n",
      "Epoch 12/100 - test accuracy: 79.52% and CE loss 0.57\n",
      "Epoch 13/100 - test accuracy: 73.12% and CE loss 0.70\n",
      "Epoch 14/100 - test accuracy: 78.83% and CE loss 0.63\n",
      "Epoch 15/100 - test accuracy: 81.32% and CE loss 0.80\n",
      "Epoch 16/100 - test accuracy: 79.31% and CE loss 0.59\n",
      "Epoch 17/100 - test accuracy: 78.73% and CE loss 0.47\n",
      "Epoch 18/100 - test accuracy: 81.81% and CE loss 0.77\n",
      "Epoch 19/100 - test accuracy: 77.45% and CE loss 0.67\n",
      "Epoch 20/100 - test accuracy: 81.71% and CE loss 0.35\n",
      "Epoch 21/100 - test accuracy: 82.62% and CE loss 0.73\n",
      "Epoch 22/100 - test accuracy: 80.31% and CE loss 0.45\n",
      "Epoch 23/100 - test accuracy: 83.06% and CE loss 0.60\n",
      "Epoch 24/100 - test accuracy: 82.73% and CE loss 0.43\n",
      "Epoch 25/100 - test accuracy: 81.55% and CE loss 0.58\n",
      "Epoch 26/100 - test accuracy: 83.24% and CE loss 0.40\n",
      "Epoch 27/100 - test accuracy: 83.72% and CE loss 0.37\n",
      "Epoch 28/100 - test accuracy: 80.71% and CE loss 0.52\n",
      "Epoch 29/100 - test accuracy: 82.04% and CE loss 0.38\n",
      "Epoch 30/100 - test accuracy: 84.56% and CE loss 0.50\n",
      "Epoch 31/100 - test accuracy: 83.72% and CE loss 0.43\n",
      "Epoch 32/100 - test accuracy: 84.69% and CE loss 0.49\n",
      "Epoch 33/100 - test accuracy: 84.15% and CE loss 0.58\n",
      "Epoch 34/100 - test accuracy: 83.29% and CE loss 0.42\n",
      "Epoch 35/100 - test accuracy: 85.61% and CE loss 0.20\n",
      "Epoch 36/100 - test accuracy: 84.89% and CE loss 0.45\n",
      "Epoch 37/100 - test accuracy: 84.13% and CE loss 0.74\n",
      "Epoch 38/100 - test accuracy: 84.41% and CE loss 0.33\n",
      "Epoch 39/100 - test accuracy: 85.78% and CE loss 0.37\n",
      "Epoch 40/100 - test accuracy: 84.59% and CE loss 0.41\n",
      "Epoch 41/100 - test accuracy: 85.27% and CE loss 0.32\n",
      "Epoch 42/100 - test accuracy: 85.96% and CE loss 0.44\n",
      "Epoch 43/100 - test accuracy: 84.54% and CE loss 0.47\n",
      "Epoch 44/100 - test accuracy: 84.89% and CE loss 0.43\n",
      "Epoch 45/100 - test accuracy: 85.30% and CE loss 0.35\n",
      "Epoch 46/100 - test accuracy: 84.38% and CE loss 0.37\n",
      "Epoch 47/100 - test accuracy: 85.22% and CE loss 0.25\n",
      "Epoch 48/100 - test accuracy: 84.66% and CE loss 0.52\n",
      "Epoch 49/100 - test accuracy: 85.10% and CE loss 0.43\n",
      "Epoch 50/100 - test accuracy: 85.68% and CE loss 0.30\n",
      "Epoch 51/100 - test accuracy: 86.65% and CE loss 0.22\n",
      "Epoch 52/100 - test accuracy: 84.23% and CE loss 0.27\n",
      "Epoch 53/100 - test accuracy: 84.94% and CE loss 0.32\n",
      "Epoch 54/100 - test accuracy: 85.58% and CE loss 0.28\n",
      "Epoch 55/100 - test accuracy: 84.82% and CE loss 0.23\n",
      "Epoch 56/100 - test accuracy: 85.89% and CE loss 0.25\n",
      "Epoch 57/100 - test accuracy: 85.53% and CE loss 0.24\n",
      "Epoch 58/100 - test accuracy: 85.10% and CE loss 0.22\n",
      "Epoch 59/100 - test accuracy: 86.14% and CE loss 0.41\n",
      "Epoch 60/100 - test accuracy: 85.99% and CE loss 0.43\n",
      "Epoch 61/100 - test accuracy: 86.09% and CE loss 0.26\n",
      "Epoch 62/100 - test accuracy: 86.09% and CE loss 0.24\n",
      "Epoch 63/100 - test accuracy: 86.78% and CE loss 0.40\n",
      "Epoch 64/100 - test accuracy: 85.58% and CE loss 0.15\n",
      "Epoch 65/100 - test accuracy: 85.76% and CE loss 0.30\n",
      "Epoch 66/100 - test accuracy: 87.18% and CE loss 0.37\n",
      "Epoch 67/100 - test accuracy: 87.24% and CE loss 0.17\n",
      "Epoch 68/100 - test accuracy: 85.63% and CE loss 0.32\n",
      "Epoch 69/100 - test accuracy: 86.04% and CE loss 0.32\n",
      "Epoch 70/100 - test accuracy: 85.20% and CE loss 0.10\n",
      "Epoch 71/100 - test accuracy: 85.58% and CE loss 0.48\n",
      "Epoch 72/100 - test accuracy: 87.03% and CE loss 0.27\n",
      "Epoch 73/100 - test accuracy: 86.98% and CE loss 0.29\n",
      "Epoch 74/100 - test accuracy: 86.70% and CE loss 0.14\n",
      "Epoch 75/100 - test accuracy: 86.93% and CE loss 0.47\n",
      "Epoch 76/100 - test accuracy: 87.26% and CE loss 0.34\n",
      "Epoch 77/100 - test accuracy: 86.93% and CE loss 0.26\n",
      "Epoch 78/100 - test accuracy: 87.57% and CE loss 0.15\n",
      "Epoch 79/100 - test accuracy: 85.78% and CE loss 0.30\n",
      "Epoch 80/100 - test accuracy: 85.94% and CE loss 0.22\n",
      "Epoch 81/100 - test accuracy: 86.96% and CE loss 0.31\n",
      "Epoch 82/100 - test accuracy: 86.96% and CE loss 0.16\n",
      "Epoch 83/100 - test accuracy: 86.83% and CE loss 0.10\n",
      "Epoch 84/100 - test accuracy: 86.88% and CE loss 0.34\n",
      "Epoch 85/100 - test accuracy: 87.16% and CE loss 0.20\n",
      "Epoch 86/100 - test accuracy: 86.93% and CE loss 0.45\n",
      "Epoch 87/100 - test accuracy: 87.08% and CE loss 0.31\n",
      "Epoch 88/100 - test accuracy: 85.96% and CE loss 0.33\n",
      "Epoch 89/100 - test accuracy: 86.27% and CE loss 0.15\n",
      "Epoch 90/100 - test accuracy: 85.91% and CE loss 0.34\n",
      "Epoch 91/100 - test accuracy: 86.96% and CE loss 0.17\n",
      "Epoch 92/100 - test accuracy: 87.24% and CE loss 0.23\n",
      "Epoch 93/100 - test accuracy: 86.78% and CE loss 0.31\n",
      "Epoch 94/100 - test accuracy: 85.63% and CE loss 0.30\n",
      "Epoch 95/100 - test accuracy: 86.93% and CE loss 0.31\n",
      "Epoch 96/100 - test accuracy: 87.39% and CE loss 0.17\n",
      "Epoch 97/100 - test accuracy: 85.86% and CE loss 0.17\n",
      "Epoch 98/100 - test accuracy: 87.18% and CE loss 0.22\n",
      "Epoch 99/100 - test accuracy: 87.72% and CE loss 0.23\n",
      "Epoch 100/100 - test accuracy: 85.76% and CE loss 0.11\n"
     ]
    }
   ],
   "source": [
    "model_global = train_global_pruned(model = model.to(device), epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "11c69e52-292b-4623-b608-4a80a76bcfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test accuracy: 85.76%\n"
     ]
    }
   ],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model_global.to(device)\n",
    "model_global.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model_global, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Model test accuracy: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "3b840659-d734-4970-b9ea-d63cbd12bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model: models\\resnet_imagenette_global.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"resnet_imagenette_global.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_global.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80e8c7-7d28-41b3-930a-01e7ec9b4dda",
   "metadata": {},
   "source": [
    "# Layered Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "d6449d31-c3fc-49de-ad38-2d8ade7901e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n"
     ]
    }
   ],
   "source": [
    "model = resnet18_features(pretrained=False, filter='None', filter_layer=0)\n",
    "learning_rate = 1e-04\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\").cuda()\n",
    "optimizer = Ranger(model.parameters(), lr = learning_rate, eps = 1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "0635f11d-091c-4967-9b1e-e3af8d4c01d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned  RESNET-18 model has 11176842 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(model)\n",
    "print(f\"Unpruned  RESNET-18 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "eff41578-1b5d-4816-815b-9a353425dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: conv1.weight & param.shape = torch.Size([64, 3, 7, 7])\n",
      "layer.name: bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.conv1.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.0.bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.conv2.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.0.bn2.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.0.bn2.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.conv1.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.1.bn1.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.bn1.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.conv2.weight & param.shape = torch.Size([64, 64, 3, 3])\n",
      "layer.name: layer1.1.bn2.weight & param.shape = torch.Size([64])\n",
      "layer.name: layer1.1.bn2.bias & param.shape = torch.Size([64])\n",
      "layer.name: layer2.0.conv1.weight & param.shape = torch.Size([128, 64, 3, 3])\n",
      "layer.name: layer2.0.bn1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.bn1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.conv2.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.0.bn2.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.bn2.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.downsample.0.weight & param.shape = torch.Size([128, 64, 1, 1])\n",
      "layer.name: layer2.0.downsample.1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.0.downsample.1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.conv1.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.1.bn1.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.bn1.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.conv2.weight & param.shape = torch.Size([128, 128, 3, 3])\n",
      "layer.name: layer2.1.bn2.weight & param.shape = torch.Size([128])\n",
      "layer.name: layer2.1.bn2.bias & param.shape = torch.Size([128])\n",
      "layer.name: layer3.0.conv1.weight & param.shape = torch.Size([256, 128, 3, 3])\n",
      "layer.name: layer3.0.bn1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.bn1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.conv2.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.0.bn2.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.bn2.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.downsample.0.weight & param.shape = torch.Size([256, 128, 1, 1])\n",
      "layer.name: layer3.0.downsample.1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.0.downsample.1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.conv1.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.1.bn1.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.bn1.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.conv2.weight & param.shape = torch.Size([256, 256, 3, 3])\n",
      "layer.name: layer3.1.bn2.weight & param.shape = torch.Size([256])\n",
      "layer.name: layer3.1.bn2.bias & param.shape = torch.Size([256])\n",
      "layer.name: layer4.0.conv1.weight & param.shape = torch.Size([512, 256, 3, 3])\n",
      "layer.name: layer4.0.bn1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.bn1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.conv2.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.0.bn2.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.bn2.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.downsample.0.weight & param.shape = torch.Size([512, 256, 1, 1])\n",
      "layer.name: layer4.0.downsample.1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.0.downsample.1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.conv1.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.1.bn1.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.bn1.bias & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.conv2.weight & param.shape = torch.Size([512, 512, 3, 3])\n",
      "layer.name: layer4.1.bn2.weight & param.shape = torch.Size([512])\n",
      "layer.name: layer4.1.bn2.bias & param.shape = torch.Size([512])\n",
      "layer.name: fc.weight & param.shape = torch.Size([10, 512])\n",
      "layer.name: fc.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in model.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "a7186bd6-acff-4a1c-b093-f213a2383864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RESNET-18 global sparsity = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\" RESNET-18 global sparsity = {compute_sparsity(model):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "dec49690-3b20-4730-a38d-2481bddedfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_layered_pruned(model, epochs):\n",
    "    for iter_prune_round in range(1):\n",
    "        print(f\"\\n\\nIterative Global pruning round = {iter_prune_round + 1}\")\n",
    "        \n",
    "        # Prune layer-wise in a structured manner-\n",
    "        prune.ln_structured(model.conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer1[0].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer1[0].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer1[0].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer1[0].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer1[1].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer1[1].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer1[1].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer1[1].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer2[0].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer2[0].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer2[0].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer2[0].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer2[1].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer2[1].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer2[1].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer2[1].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer3[0].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer3[0].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer3[0].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer3[0].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer3[1].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer3[1].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer3[1].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer3[1].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer4[0].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer4[0].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer4[0].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer4[0].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer4[1].conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer4[1].bn1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        prune.ln_structured(model.layer4[1].conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        # prune.ln_structured(model.layer4[1].bn2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "\n",
    "        prune.ln_structured(model.fc, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "        \n",
    "        # Print current global sparsity level-\n",
    "        print(f\" RESNET-18 global sparsity = {compute_sparsity(model):.2f}%\")\n",
    "        \n",
    "        \n",
    "        # Fine-training loop-\n",
    "        print(\"\\nFine-tuning pruned model to recover model's performance\\n\")\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            for x_batch, y_batch in train_dataloader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(x_batch)\n",
    "                loss = criterion(logits, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "            # Evaluate model!\n",
    "            if epochs%10==0:\n",
    "                predictions, labels = evaluate_model(model, test_dataloader, device)\n",
    "                test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - test accuracy: {(100 * test_acc):.2f}% and CE loss {loss.item():.2f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "5ae15b2d-4fa5-4b34-8147-5a8c7c368e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iterative Global pruning round = 1\n",
      " RESNET-18 global sparsity = 9.99%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch 1/100 - test accuracy: 27.92% and CE loss 2.22\n",
      "Epoch 2/100 - test accuracy: 46.96% and CE loss 1.55\n",
      "Epoch 3/100 - test accuracy: 54.90% and CE loss 1.63\n",
      "Epoch 4/100 - test accuracy: 57.99% and CE loss 1.32\n",
      "Epoch 5/100 - test accuracy: 61.32% and CE loss 1.21\n",
      "Epoch 6/100 - test accuracy: 61.35% and CE loss 1.34\n",
      "Epoch 7/100 - test accuracy: 63.92% and CE loss 1.27\n",
      "Epoch 8/100 - test accuracy: 65.78% and CE loss 1.12\n",
      "Epoch 9/100 - test accuracy: 67.24% and CE loss 1.24\n",
      "Epoch 10/100 - test accuracy: 67.26% and CE loss 1.09\n",
      "Epoch 11/100 - test accuracy: 68.66% and CE loss 1.07\n",
      "Epoch 12/100 - test accuracy: 69.68% and CE loss 1.02\n",
      "Epoch 13/100 - test accuracy: 70.90% and CE loss 1.04\n",
      "Epoch 14/100 - test accuracy: 70.85% and CE loss 1.07\n",
      "Epoch 15/100 - test accuracy: 71.18% and CE loss 0.58\n",
      "Epoch 16/100 - test accuracy: 71.75% and CE loss 0.98\n",
      "Epoch 17/100 - test accuracy: 73.63% and CE loss 0.87\n",
      "Epoch 18/100 - test accuracy: 73.04% and CE loss 0.69\n",
      "Epoch 19/100 - test accuracy: 72.79% and CE loss 1.12\n",
      "Epoch 20/100 - test accuracy: 74.24% and CE loss 0.62\n",
      "Epoch 21/100 - test accuracy: 75.82% and CE loss 0.60\n",
      "Epoch 22/100 - test accuracy: 73.91% and CE loss 0.86\n",
      "Epoch 23/100 - test accuracy: 75.59% and CE loss 0.63\n",
      "Epoch 24/100 - test accuracy: 75.59% and CE loss 0.79\n",
      "Epoch 25/100 - test accuracy: 74.47% and CE loss 0.96\n",
      "Epoch 26/100 - test accuracy: 75.54% and CE loss 0.81\n",
      "Epoch 27/100 - test accuracy: 77.25% and CE loss 0.56\n",
      "Epoch 28/100 - test accuracy: 75.62% and CE loss 0.69\n",
      "Epoch 29/100 - test accuracy: 76.41% and CE loss 0.53\n",
      "Epoch 30/100 - test accuracy: 77.25% and CE loss 0.54\n",
      "Epoch 31/100 - test accuracy: 75.90% and CE loss 0.60\n",
      "Epoch 32/100 - test accuracy: 77.45% and CE loss 0.54\n",
      "Epoch 33/100 - test accuracy: 78.68% and CE loss 0.49\n",
      "Epoch 34/100 - test accuracy: 77.35% and CE loss 0.73\n",
      "Epoch 35/100 - test accuracy: 78.47% and CE loss 0.67\n",
      "Epoch 36/100 - test accuracy: 79.44% and CE loss 0.61\n",
      "Epoch 37/100 - test accuracy: 78.93% and CE loss 0.64\n",
      "Epoch 38/100 - test accuracy: 76.92% and CE loss 0.49\n",
      "Epoch 39/100 - test accuracy: 77.55% and CE loss 0.61\n",
      "Epoch 40/100 - test accuracy: 78.68% and CE loss 0.61\n",
      "Epoch 41/100 - test accuracy: 78.50% and CE loss 0.76\n",
      "Epoch 42/100 - test accuracy: 79.54% and CE loss 0.51\n",
      "Epoch 43/100 - test accuracy: 77.91% and CE loss 0.51\n",
      "Epoch 44/100 - test accuracy: 80.15% and CE loss 0.45\n",
      "Epoch 45/100 - test accuracy: 80.10% and CE loss 0.41\n",
      "Epoch 46/100 - test accuracy: 79.82% and CE loss 0.61\n",
      "Epoch 47/100 - test accuracy: 79.87% and CE loss 0.45\n",
      "Epoch 48/100 - test accuracy: 79.49% and CE loss 0.62\n",
      "Epoch 49/100 - test accuracy: 79.26% and CE loss 0.26\n",
      "Epoch 50/100 - test accuracy: 79.11% and CE loss 0.48\n",
      "Epoch 51/100 - test accuracy: 80.54% and CE loss 0.24\n",
      "Epoch 52/100 - test accuracy: 78.47% and CE loss 0.30\n",
      "Epoch 53/100 - test accuracy: 79.75% and CE loss 0.13\n",
      "Epoch 54/100 - test accuracy: 80.41% and CE loss 0.52\n",
      "Epoch 55/100 - test accuracy: 79.85% and CE loss 0.36\n",
      "Epoch 56/100 - test accuracy: 80.13% and CE loss 0.52\n",
      "Epoch 57/100 - test accuracy: 79.90% and CE loss 0.41\n",
      "Epoch 58/100 - test accuracy: 79.16% and CE loss 0.45\n",
      "Epoch 59/100 - test accuracy: 80.46% and CE loss 0.32\n",
      "Epoch 60/100 - test accuracy: 80.82% and CE loss 0.42\n",
      "Epoch 61/100 - test accuracy: 79.26% and CE loss 0.48\n",
      "Epoch 62/100 - test accuracy: 79.82% and CE loss 0.54\n",
      "Epoch 63/100 - test accuracy: 81.35% and CE loss 0.39\n",
      "Epoch 64/100 - test accuracy: 80.43% and CE loss 0.59\n",
      "Epoch 65/100 - test accuracy: 80.97% and CE loss 0.27\n",
      "Epoch 66/100 - test accuracy: 81.71% and CE loss 0.35\n",
      "Epoch 67/100 - test accuracy: 79.49% and CE loss 0.28\n",
      "Epoch 68/100 - test accuracy: 79.67% and CE loss 0.31\n",
      "Epoch 69/100 - test accuracy: 81.81% and CE loss 0.49\n",
      "Epoch 70/100 - test accuracy: 80.08% and CE loss 0.42\n",
      "Epoch 71/100 - test accuracy: 81.02% and CE loss 0.23\n",
      "Epoch 72/100 - test accuracy: 81.66% and CE loss 0.28\n",
      "Epoch 73/100 - test accuracy: 79.16% and CE loss 0.54\n",
      "Epoch 74/100 - test accuracy: 81.66% and CE loss 0.25\n",
      "Epoch 75/100 - test accuracy: 80.76% and CE loss 0.22\n",
      "Epoch 76/100 - test accuracy: 80.28% and CE loss 0.22\n",
      "Epoch 77/100 - test accuracy: 80.71% and CE loss 0.55\n",
      "Epoch 78/100 - test accuracy: 80.84% and CE loss 0.28\n",
      "Epoch 79/100 - test accuracy: 79.59% and CE loss 0.26\n",
      "Epoch 80/100 - test accuracy: 81.27% and CE loss 0.23\n",
      "Epoch 81/100 - test accuracy: 81.76% and CE loss 0.20\n",
      "Epoch 82/100 - test accuracy: 80.59% and CE loss 0.15\n",
      "Epoch 83/100 - test accuracy: 82.37% and CE loss 0.25\n",
      "Epoch 84/100 - test accuracy: 82.29% and CE loss 0.40\n",
      "Epoch 85/100 - test accuracy: 80.61% and CE loss 0.33\n",
      "Epoch 86/100 - test accuracy: 81.27% and CE loss 0.17\n",
      "Epoch 87/100 - test accuracy: 80.89% and CE loss 0.46\n",
      "Epoch 88/100 - test accuracy: 80.41% and CE loss 0.19\n",
      "Epoch 89/100 - test accuracy: 81.07% and CE loss 0.33\n",
      "Epoch 90/100 - test accuracy: 82.14% and CE loss 0.24\n",
      "Epoch 91/100 - test accuracy: 80.43% and CE loss 0.26\n",
      "Epoch 92/100 - test accuracy: 81.83% and CE loss 0.20\n",
      "Epoch 93/100 - test accuracy: 81.78% and CE loss 0.27\n",
      "Epoch 94/100 - test accuracy: 81.35% and CE loss 0.42\n",
      "Epoch 95/100 - test accuracy: 80.89% and CE loss 0.26\n",
      "Epoch 96/100 - test accuracy: 81.17% and CE loss 0.34\n",
      "Epoch 97/100 - test accuracy: 81.68% and CE loss 0.47\n",
      "Epoch 98/100 - test accuracy: 82.24% and CE loss 0.11\n",
      "Epoch 99/100 - test accuracy: 81.73% and CE loss 0.22\n",
      "Epoch 100/100 - test accuracy: 80.64% and CE loss 0.26\n"
     ]
    }
   ],
   "source": [
    "model_layered_structured = train_layered_pruned(model = model.to(device), epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "70fa8af2-e65a-4c1f-b967-fc9bf24e65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model test accuracy: 80.64%\n"
     ]
    }
   ],
   "source": [
    "# Model to GPU and eval mode.\n",
    "model_layered_structured.to(device)\n",
    "model_layered_structured.eval()\n",
    "\n",
    "# Check test set performance.\n",
    "predictions, labels = evaluate_model(model_layered_structured, test_dataloader, device)\n",
    "test_acc = np.mean(np.argmax(predictions.cpu().numpy(), axis=1) == labels.cpu().numpy())        \n",
    "print(f\"Model test accuracy: {(100 * test_acc):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "92f17f76-3bff-4b66-8898-b59ec760fcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the model: models\\resnet_imagenette_structured.pth\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"resnet_imagenette_structured.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "print(f\"Saving the model: {MODEL_SAVE_PATH}\")\n",
    "torch.save(obj=model_layered_structured.state_dict(), f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a21cf-fed1-4a30-a24c-7fe07a6983dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
