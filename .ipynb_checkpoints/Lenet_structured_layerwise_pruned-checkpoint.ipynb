{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e955b499-d0d7-4620-8253-08a026795785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cea03925-7440-4c0a-84fa-c4bb25e6e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7021bbc0-cf85-4f0b-acb8-e3b1b11ef228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root=\"./datasets\", train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdea28bd-9584-42c2-afb1-686bdcc890e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1307]), tensor([0.3081]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "imgs = torch.stack([img for img, _ in train_val_dataset], dim =0)\n",
    "mean = imgs.view(1, -1).mean(dim =1)\n",
    "std = imgs.view(1, -1).std(dim =1)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4542af07-1954-48ee-818f-9993d76f7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=mnist_transforms)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f652e213-d1b8-4f38-b89c-0d1326bb8fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 6000, 10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.9 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6ff040-4ebb-4db0-8ab8-dfdc3b12c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 188, 313)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Let's see no of batches that we have now with the current batch-size\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9270fe04-967c-47e1-98b6-3d40561c98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5V1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d32c3791-2671-4a62-a5f5-a19faf793a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "model_lenet5v1 = LeNet5V1()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_lenet5v1.parameters(), lr = 0.001)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1ed1f5c-a733-44e3-b86b-cc252693096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212820e37bbf4d308d046a76b106745a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train loss:  0.19812| Train acc:  0.93943| Val loss:  0.05795| Val acc:  0.98155\n",
      "Epoch: 1| Train loss:  0.08698| Train acc:  0.97438| Val loss:  0.05398| Val acc:  0.98321\n",
      "Epoch: 2| Train loss:  0.06786| Train acc:  0.97932| Val loss:  0.04531| Val acc:  0.98604\n",
      "Epoch: 3| Train loss:  0.05601| Train acc:  0.98306| Val loss:  0.03791| Val acc:  0.98787\n",
      "Epoch: 4| Train loss:  0.04731| Train acc:  0.98467| Val loss:  0.03715| Val acc:  0.98920\n",
      "Epoch: 5| Train loss:  0.04003| Train acc:  0.98778| Val loss:  0.04136| Val acc:  0.98853\n",
      "Epoch: 6| Train loss:  0.03599| Train acc:  0.98861| Val loss:  0.04293| Val acc:  0.98853\n",
      "Epoch: 7| Train loss:  0.03512| Train acc:  0.98904| Val loss:  0.03654| Val acc:  0.99086\n",
      "Epoch: 8| Train loss:  0.02981| Train acc:  0.99037| Val loss:  0.03441| Val acc:  0.99069\n",
      "Epoch: 9| Train loss:  0.02534| Train acc:  0.99209| Val loss:  0.05111| Val acc:  0.98903\n",
      "Epoch: 10| Train loss:  0.02590| Train acc:  0.99165| Val loss:  0.04438| Val acc:  0.99102\n",
      "Epoch: 11| Train loss:  0.02402| Train acc:  0.99248| Val loss:  0.04962| Val acc:  0.98969\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# device-agnostic setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy = accuracy.to(device)\n",
    "model_lenet5v1 = model_lenet5v1.to(device)\n",
    "\n",
    "EPOCHS = 12\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Training loop\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        model_lenet5v1.train()\n",
    "        \n",
    "        y_pred = model_lenet5v1(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "        \n",
    "    # Validation loop\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model_lenet5v1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            val_acc += acc\n",
    "            \n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37658970-2e82-405f-8699-4ba457d464a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    total_params = 0\n",
    "    for layer_names, param in model.named_parameters():\n",
    "        total_params += torch.count_nonzero(param.data)\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7184f902-044e-4cb8-a5b3-4f1a484941b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned LeNet-5 model has 1199882 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(model_lenet5v1)\n",
    "print(f\"Unpruned LeNet-5 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec4a58de-1685-43b4-b644-91ba3db59438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: conv1.weight & param.shape = torch.Size([32, 1, 3, 3])\n",
      "layer.name: conv1.bias & param.shape = torch.Size([32])\n",
      "layer.name: conv2.weight & param.shape = torch.Size([64, 32, 3, 3])\n",
      "layer.name: conv2.bias & param.shape = torch.Size([64])\n",
      "layer.name: fc1.weight & param.shape = torch.Size([128, 9216])\n",
      "layer.name: fc1.bias & param.shape = torch.Size([128])\n",
      "layer.name: fc2.weight & param.shape = torch.Size([10, 128])\n",
      "layer.name: fc2.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in model_lenet5v1.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e89617fc-d894-43e1-a737-29cecf25b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([32, 1, 3, 3])\n",
      "conv1.bias torch.Size([32])\n",
      "conv2.weight torch.Size([64, 32, 3, 3])\n",
      "conv2.bias torch.Size([64])\n",
      "fc1.weight torch.Size([128, 9216])\n",
      "fc1.bias torch.Size([128])\n",
      "fc2.weight torch.Size([10, 128])\n",
      "fc2.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer_name in model_lenet5v1.state_dict().keys():\n",
    "    print(layer_name, model_lenet5v1.state_dict()[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "367d8c12-508c-467f-a544-49ac4bf7821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lenet5v1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a5380d8-3a4b-4bbc-8328-3a710134c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(model):\n",
    "    conv1_sparsity = (torch.sum(model.conv1.weight == 0) / model.conv1.weight.nelement()) * 100\n",
    "    conv2_sparsity = (torch.sum(model.conv2.weight == 0) / model.conv2.weight.nelement()) * 100\n",
    "    fc1_sparsity = (torch.sum(model.fc1.weight == 0) / model.fc1.weight.nelement()) * 100\n",
    "    op_sparsity = (torch.sum(model.fc2.weight == 0) / model.fc2.weight.nelement()) * 100\n",
    "\n",
    "    num = torch.sum(model.conv1.weight == 0) + torch.sum(model.conv2.weight == 0) + torch.sum(model.fc1.weight == 0) + torch.sum(model.fc2.weight == 0)\n",
    "    denom = model.conv1.weight.nelement() + model.conv2.weight.nelement() + model.fc1.weight.nelement() + model.fc2.weight.nelement()\n",
    "\n",
    "    global_sparsity = num/denom * 100\n",
    "\n",
    "    return global_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cbeb79bc-735b-4371-b53a-f91a14ac2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5 global sparsity = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"LeNet-5 global sparsity = {compute_sparsity(model_lenet5v1):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0518e81c-0c47-49be-8719-2e396696fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iterative Global pruning round = 1\n",
      "LeNet-5 global sparsity = 10.14%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.02937| Train acc:  0.99072| Val loss:  0.04318| Val acc:  0.98953\n",
      "Epoch: 1| Train loss:  0.02305| Train acc:  0.99265| Val loss:  0.03614| Val acc:  0.99053\n",
      "Epoch: 2| Train loss:  0.02159| Train acc:  0.99309| Val loss:  0.03595| Val acc:  0.99136\n",
      "Epoch: 3| Train loss:  0.01749| Train acc:  0.99441| Val loss:  0.04063| Val acc:  0.99152\n",
      "Epoch: 4| Train loss:  0.02059| Train acc:  0.99345| Val loss:  0.03938| Val acc:  0.99136\n",
      "Epoch: 5| Train loss:  0.01995| Train acc:  0.99422| Val loss:  0.04305| Val acc:  0.99036\n",
      "Epoch: 6| Train loss:  0.01866| Train acc:  0.99411| Val loss:  0.03832| Val acc:  0.99252\n",
      "Epoch: 7| Train loss:  0.01624| Train acc:  0.99472| Val loss:  0.04259| Val acc:  0.99053\n",
      "Epoch: 8| Train loss:  0.01731| Train acc:  0.99496| Val loss:  0.03778| Val acc:  0.99169\n",
      "Epoch: 9| Train loss:  0.01529| Train acc:  0.99530| Val loss:  0.03976| Val acc:  0.99102\n",
      "Epoch: 10| Train loss:  0.01528| Train acc:  0.99506| Val loss:  0.04202| Val acc:  0.99136\n",
      "Epoch: 11| Train loss:  0.01483| Train acc:  0.99554| Val loss:  0.03738| Val acc:  0.99152\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 2\n",
      "LeNet-5 global sparsity = 19.52%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.15434| Train acc:  0.89709| Val loss:  0.17124| Val acc:  0.89495\n",
      "Epoch: 1| Train loss:  0.15122| Train acc:  0.89823| Val loss:  0.17065| Val acc:  0.89594\n",
      "Epoch: 2| Train loss:  0.15094| Train acc:  0.89718| Val loss:  0.17226| Val acc:  0.89528\n",
      "Epoch: 3| Train loss:  0.15030| Train acc:  0.89777| Val loss:  0.17503| Val acc:  0.89545\n",
      "Epoch: 4| Train loss:  0.15060| Train acc:  0.89799| Val loss:  0.17296| Val acc:  0.89478\n",
      "Epoch: 5| Train loss:  0.14824| Train acc:  0.89853| Val loss:  0.17112| Val acc:  0.89611\n",
      "Epoch: 6| Train loss:  0.14839| Train acc:  0.89870| Val loss:  0.17353| Val acc:  0.89495\n",
      "Epoch: 7| Train loss:  0.14847| Train acc:  0.89823| Val loss:  0.17328| Val acc:  0.89578\n",
      "Epoch: 8| Train loss:  0.14742| Train acc:  0.89909| Val loss:  0.17552| Val acc:  0.89578\n",
      "Epoch: 9| Train loss:  0.14904| Train acc:  0.89836| Val loss:  0.17616| Val acc:  0.89445\n",
      "Epoch: 10| Train loss:  0.14795| Train acc:  0.89875| Val loss:  0.17728| Val acc:  0.89578\n",
      "Epoch: 11| Train loss:  0.14826| Train acc:  0.89807| Val loss:  0.18656| Val acc:  0.89445\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 3\n",
      "LeNet-5 global sparsity = 27.33%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.34229| Train acc:  0.79915| Val loss:  0.35770| Val acc:  0.79771\n",
      "Epoch: 1| Train loss:  0.33634| Train acc:  0.80174| Val loss:  0.36667| Val acc:  0.79604\n",
      "Epoch: 2| Train loss:  0.33565| Train acc:  0.79947| Val loss:  0.35913| Val acc:  0.79887\n",
      "Epoch: 3| Train loss:  0.33373| Train acc:  0.80248| Val loss:  0.35532| Val acc:  0.79837\n",
      "Epoch: 4| Train loss:  0.33621| Train acc:  0.80037| Val loss:  0.35596| Val acc:  0.79837\n",
      "Epoch: 5| Train loss:  0.33390| Train acc:  0.80134| Val loss:  0.35479| Val acc:  0.79837\n",
      "Epoch: 6| Train loss:  0.33453| Train acc:  0.80137| Val loss:  0.34972| Val acc:  0.79937\n",
      "Epoch: 7| Train loss:  0.33301| Train acc:  0.80061| Val loss:  0.36513| Val acc:  0.79754\n",
      "Epoch: 8| Train loss:  0.33339| Train acc:  0.80071| Val loss:  0.36293| Val acc:  0.79787\n",
      "Epoch: 9| Train loss:  0.33450| Train acc:  0.80045| Val loss:  0.36293| Val acc:  0.79870\n",
      "Epoch: 10| Train loss:  0.33478| Train acc:  0.80317| Val loss:  0.35636| Val acc:  0.79904\n",
      "Epoch: 11| Train loss:  0.33266| Train acc:  0.80206| Val loss:  0.36045| Val acc:  0.79804\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 4\n",
      "LeNet-5 global sparsity = 34.38%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.54968| Train acc:  0.70825| Val loss:  0.56569| Val acc:  0.70944\n",
      "Epoch: 1| Train loss:  0.54816| Train acc:  0.71116| Val loss:  0.56288| Val acc:  0.71011\n",
      "Epoch: 2| Train loss:  0.54564| Train acc:  0.71138| Val loss:  0.56268| Val acc:  0.71144\n",
      "Epoch: 3| Train loss:  0.54548| Train acc:  0.71036| Val loss:  0.56381| Val acc:  0.71061\n",
      "Epoch: 4| Train loss:  0.54282| Train acc:  0.71147| Val loss:  0.57090| Val acc:  0.70977\n",
      "Epoch: 5| Train loss:  0.54466| Train acc:  0.71122| Val loss:  0.56395| Val acc:  0.71061\n",
      "Epoch: 6| Train loss:  0.54435| Train acc:  0.71160| Val loss:  0.56240| Val acc:  0.70994\n",
      "Epoch: 7| Train loss:  0.54073| Train acc:  0.71259| Val loss:  0.56832| Val acc:  0.71011\n",
      "Epoch: 8| Train loss:  0.54387| Train acc:  0.71103| Val loss:  0.56725| Val acc:  0.71011\n",
      "Epoch: 9| Train loss:  0.54354| Train acc:  0.71099| Val loss:  0.56870| Val acc:  0.71110\n",
      "Epoch: 10| Train loss:  0.54317| Train acc:  0.71199| Val loss:  0.55807| Val acc:  0.71127\n",
      "Epoch: 11| Train loss:  0.54279| Train acc:  0.71103| Val loss:  0.56382| Val acc:  0.71077\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 5\n",
      "LeNet-5 global sparsity = 40.64%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80667| Train acc:  0.60912| Val loss:  0.81419| Val acc:  0.60788\n",
      "Epoch: 1| Train loss:  0.79467| Train acc:  0.61032| Val loss:  0.80580| Val acc:  0.61287\n",
      "Epoch: 2| Train loss:  0.79220| Train acc:  0.60978| Val loss:  0.81872| Val acc:  0.61137\n",
      "Epoch: 3| Train loss:  0.79108| Train acc:  0.61204| Val loss:  0.81106| Val acc:  0.60821\n",
      "Epoch: 4| Train loss:  0.79165| Train acc:  0.61139| Val loss:  0.81449| Val acc:  0.60688\n",
      "Epoch: 5| Train loss:  0.78979| Train acc:  0.61424| Val loss:  0.80998| Val acc:  0.61270\n",
      "Epoch: 6| Train loss:  0.78986| Train acc:  0.61454| Val loss:  0.81078| Val acc:  0.60954\n",
      "Epoch: 7| Train loss:  0.78919| Train acc:  0.61317| Val loss:  0.81215| Val acc:  0.60921\n",
      "Epoch: 8| Train loss:  0.78988| Train acc:  0.61482| Val loss:  0.81527| Val acc:  0.61237\n",
      "Epoch: 9| Train loss:  0.78968| Train acc:  0.61199| Val loss:  0.81398| Val acc:  0.61287\n",
      "Epoch: 10| Train loss:  0.78810| Train acc:  0.61178| Val loss:  0.81245| Val acc:  0.60771\n",
      "Epoch: 11| Train loss:  0.78808| Train acc:  0.61256| Val loss:  0.81237| Val acc:  0.60821\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 6\n",
      "LeNet-5 global sparsity = 46.88%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.79536| Train acc:  0.60945| Val loss:  0.81955| Val acc:  0.60788\n",
      "Epoch: 1| Train loss:  0.79428| Train acc:  0.61039| Val loss:  0.81769| Val acc:  0.60805\n",
      "Epoch: 2| Train loss:  0.79404| Train acc:  0.61189| Val loss:  0.81210| Val acc:  0.60854\n",
      "Epoch: 3| Train loss:  0.79104| Train acc:  0.61093| Val loss:  0.81703| Val acc:  0.60771\n",
      "Epoch: 4| Train loss:  0.79061| Train acc:  0.61363| Val loss:  0.81548| Val acc:  0.61187\n",
      "Epoch: 5| Train loss:  0.79161| Train acc:  0.61245| Val loss:  0.81348| Val acc:  0.61220\n",
      "Epoch: 6| Train loss:  0.79045| Train acc:  0.61315| Val loss:  0.81008| Val acc:  0.60838\n",
      "Epoch: 7| Train loss:  0.78822| Train acc:  0.61217| Val loss:  0.81892| Val acc:  0.60821\n",
      "Epoch: 8| Train loss:  0.78991| Train acc:  0.61106| Val loss:  0.81834| Val acc:  0.61237\n",
      "Epoch: 9| Train loss:  0.78869| Train acc:  0.61193| Val loss:  0.81648| Val acc:  0.61270\n",
      "Epoch: 10| Train loss:  0.79041| Train acc:  0.61261| Val loss:  0.81550| Val acc:  0.60805\n",
      "Epoch: 11| Train loss:  0.78989| Train acc:  0.61089| Val loss:  0.81661| Val acc:  0.60788\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 7\n",
      "LeNet-5 global sparsity = 52.33%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80736| Train acc:  0.61102| Val loss:  0.81808| Val acc:  0.60638\n",
      "Epoch: 1| Train loss:  0.79548| Train acc:  0.60926| Val loss:  0.81872| Val acc:  0.61104\n",
      "Epoch: 2| Train loss:  0.79497| Train acc:  0.61200| Val loss:  0.81923| Val acc:  0.61187\n",
      "Epoch: 3| Train loss:  0.79438| Train acc:  0.61113| Val loss:  0.81428| Val acc:  0.60755\n",
      "Epoch: 4| Train loss:  0.79445| Train acc:  0.60975| Val loss:  0.81910| Val acc:  0.60838\n",
      "Epoch: 5| Train loss:  0.79401| Train acc:  0.61252| Val loss:  0.81364| Val acc:  0.61287\n",
      "Epoch: 6| Train loss:  0.79359| Train acc:  0.61258| Val loss:  0.81164| Val acc:  0.60771\n",
      "Epoch: 7| Train loss:  0.79476| Train acc:  0.61130| Val loss:  0.81053| Val acc:  0.61303\n",
      "Epoch: 8| Train loss:  0.79282| Train acc:  0.61158| Val loss:  0.81234| Val acc:  0.60904\n",
      "Epoch: 9| Train loss:  0.79148| Train acc:  0.61095| Val loss:  0.81512| Val acc:  0.61287\n",
      "Epoch: 10| Train loss:  0.79116| Train acc:  0.60788| Val loss:  0.82103| Val acc:  0.60838\n",
      "Epoch: 11| Train loss:  0.79124| Train acc:  0.61245| Val loss:  0.82310| Val acc:  0.61237\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 8\n",
      "LeNet-5 global sparsity = 57.01%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80913| Train acc:  0.61030| Val loss:  0.82778| Val acc:  0.60672\n",
      "Epoch: 1| Train loss:  0.80540| Train acc:  0.61000| Val loss:  0.81913| Val acc:  0.60721\n",
      "Epoch: 2| Train loss:  0.79830| Train acc:  0.61004| Val loss:  0.81793| Val acc:  0.60688\n",
      "Epoch: 3| Train loss:  0.79841| Train acc:  0.61019| Val loss:  0.81882| Val acc:  0.60771\n",
      "Epoch: 4| Train loss:  0.79815| Train acc:  0.60917| Val loss:  0.81966| Val acc:  0.60721\n",
      "Epoch: 5| Train loss:  0.79815| Train acc:  0.61165| Val loss:  0.81586| Val acc:  0.61187\n",
      "Epoch: 6| Train loss:  0.79798| Train acc:  0.61223| Val loss:  0.82267| Val acc:  0.60705\n",
      "Epoch: 7| Train loss:  0.79478| Train acc:  0.61060| Val loss:  0.82172| Val acc:  0.61137\n",
      "Epoch: 8| Train loss:  0.79637| Train acc:  0.60980| Val loss:  0.81451| Val acc:  0.60738\n",
      "Epoch: 9| Train loss:  0.79645| Train acc:  0.61147| Val loss:  0.81717| Val acc:  0.61187\n",
      "Epoch: 10| Train loss:  0.79487| Train acc:  0.61112| Val loss:  0.81464| Val acc:  0.60738\n",
      "Epoch: 11| Train loss:  0.79528| Train acc:  0.61258| Val loss:  0.82273| Val acc:  0.61154\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 9\n",
      "LeNet-5 global sparsity = 61.69%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.81168| Train acc:  0.60945| Val loss:  0.82131| Val acc:  0.60655\n",
      "Epoch: 1| Train loss:  0.80475| Train acc:  0.60863| Val loss:  0.81454| Val acc:  0.60605\n",
      "Epoch: 2| Train loss:  0.80245| Train acc:  0.61113| Val loss:  0.81645| Val acc:  0.60771\n",
      "Epoch: 3| Train loss:  0.80274| Train acc:  0.60836| Val loss:  0.81824| Val acc:  0.60688\n",
      "Epoch: 4| Train loss:  0.80034| Train acc:  0.61045| Val loss:  0.81358| Val acc:  0.60788\n",
      "Epoch: 5| Train loss:  0.80079| Train acc:  0.60869| Val loss:  0.81465| Val acc:  0.60705\n",
      "Epoch: 6| Train loss:  0.79979| Train acc:  0.60891| Val loss:  0.82061| Val acc:  0.60638\n",
      "Epoch: 7| Train loss:  0.80016| Train acc:  0.61117| Val loss:  0.81418| Val acc:  0.60688\n",
      "Epoch: 8| Train loss:  0.79837| Train acc:  0.60980| Val loss:  0.81977| Val acc:  0.60738\n",
      "Epoch: 9| Train loss:  0.80051| Train acc:  0.61162| Val loss:  0.81629| Val acc:  0.61237\n",
      "Epoch: 10| Train loss:  0.79804| Train acc:  0.60899| Val loss:  0.81680| Val acc:  0.61187\n",
      "Epoch: 11| Train loss:  0.79870| Train acc:  0.61115| Val loss:  0.81617| Val acc:  0.61137\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 10\n",
      "LeNet-5 global sparsity = 65.58%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.81396| Train acc:  0.60710| Val loss:  0.81662| Val acc:  0.60721\n",
      "Epoch: 1| Train loss:  0.80839| Train acc:  0.60912| Val loss:  0.81716| Val acc:  0.61237\n",
      "Epoch: 2| Train loss:  0.80685| Train acc:  0.60960| Val loss:  0.81801| Val acc:  0.60688\n",
      "Epoch: 3| Train loss:  0.80740| Train acc:  0.60895| Val loss:  0.81333| Val acc:  0.60705\n",
      "Epoch: 4| Train loss:  0.80450| Train acc:  0.61015| Val loss:  0.81827| Val acc:  0.61120\n",
      "Epoch: 5| Train loss:  0.80559| Train acc:  0.61010| Val loss:  0.81882| Val acc:  0.60588\n",
      "Epoch: 6| Train loss:  0.80473| Train acc:  0.60899| Val loss:  0.81560| Val acc:  0.60755\n",
      "Epoch: 7| Train loss:  0.80267| Train acc:  0.60928| Val loss:  0.81974| Val acc:  0.60655\n",
      "Epoch: 8| Train loss:  0.80353| Train acc:  0.60882| Val loss:  0.81584| Val acc:  0.61137\n",
      "Epoch: 9| Train loss:  0.80371| Train acc:  0.60854| Val loss:  0.81859| Val acc:  0.61120\n",
      "Epoch: 10| Train loss:  0.80351| Train acc:  0.60752| Val loss:  0.82170| Val acc:  0.60688\n",
      "Epoch: 11| Train loss:  0.80156| Train acc:  0.61019| Val loss:  0.82108| Val acc:  0.60755\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "for iter_prune_round in range(10):\n",
    "    print(f\"\\n\\nIterative Global pruning round = {iter_prune_round + 1}\")\n",
    "    \n",
    "    # Prune layer-wise in a structured manner-\n",
    "    prune.ln_structured(model_lenet5v1.conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.fc1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.fc2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "\n",
    "    # Print current global sparsity level-\n",
    "    print(f\"LeNet-5 global sparsity = {compute_sparsity(model_lenet5v1):.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Fine-training loop-\n",
    "    print(\"\\nFine-tuning pruned model to recover model's performance\\n\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            model_lenet5v1.train()\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            train_acc += acc\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "            \n",
    "        # Validation loop\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        model_lenet5v1.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X, y in val_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                y_pred = model_lenet5v1(X)\n",
    "                \n",
    "                loss = loss_fn(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                acc = accuracy(y_pred, y)\n",
    "                val_acc += acc\n",
    "                \n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc /= len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691c0d6-54b7-42b3-b715-5be196b1695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
