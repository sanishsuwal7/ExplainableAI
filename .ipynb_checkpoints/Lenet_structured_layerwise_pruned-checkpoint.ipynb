{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e955b499-d0d7-4620-8253-08a026795785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cea03925-7440-4c0a-84fa-c4bb25e6e370",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7021bbc0-cf85-4f0b-acb8-e3b1b11ef228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root=\"./datasets\", train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdea28bd-9584-42c2-afb1-686bdcc890e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1307]), tensor([0.3081]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "imgs = torch.stack([img for img, _ in train_val_dataset], dim =0)\n",
    "mean = imgs.view(1, -1).mean(dim =1)\n",
    "std = imgs.view(1, -1).std(dim =1)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4542af07-1954-48ee-818f-9993d76f7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "train_val_dataset = datasets.MNIST(root=\"./datasets/\", train=True, download=False, transform=mnist_transforms)\n",
    "test_dataset = datasets.MNIST(root=\"./datasets/\", train=False, download=False, transform=mnist_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f652e213-d1b8-4f38-b89c-0d1326bb8fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 6000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = int(0.9 * len(train_val_dataset))\n",
    "val_size = len(train_val_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset=train_val_dataset, lengths=[train_size, val_size])\n",
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd6ff040-4ebb-4db0-8ab8-dfdc3b12c2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 188, 313)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Let's see no of batches that we have now with the current batch-size\n",
    "len(train_dataloader), len(val_dataloader), len(test_dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9270fe04-967c-47e1-98b6-3d40561c98f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5V1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5V1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d32c3791-2671-4a62-a5f5-a19faf793a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "model_lenet5v1 = LeNet5V1()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_lenet5v1.parameters(), lr = 0.001)\n",
    "accuracy = Accuracy(task='multiclass', num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1ed1f5c-a733-44e3-b86b-cc252693096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e9ce23b9ca498e97cf1b91ac56218d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| Train loss:  0.19825| Train acc:  0.94059| Val loss:  0.07203| Val acc:  0.97955\n",
      "Epoch: 1| Train loss:  0.08707| Train acc:  0.97367| Val loss:  0.05336| Val acc:  0.98437\n",
      "Epoch: 2| Train loss:  0.07069| Train acc:  0.97954| Val loss:  0.05107| Val acc:  0.98703\n",
      "Epoch: 3| Train loss:  0.05812| Train acc:  0.98306| Val loss:  0.05112| Val acc:  0.98604\n",
      "Epoch: 4| Train loss:  0.04928| Train acc:  0.98582| Val loss:  0.04461| Val acc:  0.98903\n",
      "Epoch: 5| Train loss:  0.04234| Train acc:  0.98699| Val loss:  0.04914| Val acc:  0.98903\n",
      "Epoch: 6| Train loss:  0.03798| Train acc:  0.98778| Val loss:  0.04580| Val acc:  0.98803\n",
      "Epoch: 7| Train loss:  0.03255| Train acc:  0.98971| Val loss:  0.05236| Val acc:  0.98836\n",
      "Epoch: 8| Train loss:  0.03072| Train acc:  0.99061| Val loss:  0.05877| Val acc:  0.98787\n",
      "Epoch: 9| Train loss:  0.02961| Train acc:  0.99032| Val loss:  0.05240| Val acc:  0.98886\n",
      "Epoch: 10| Train loss:  0.02716| Train acc:  0.99154| Val loss:  0.04985| Val acc:  0.99003\n",
      "Epoch: 11| Train loss:  0.02286| Train acc:  0.99235| Val loss:  0.05312| Val acc:  0.98920\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# device-agnostic setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "accuracy = accuracy.to(device)\n",
    "model_lenet5v1 = model_lenet5v1.to(device)\n",
    "\n",
    "EPOCHS = 12\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Training loop\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    for X, y in train_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        model_lenet5v1.train()\n",
    "        \n",
    "        y_pred = model_lenet5v1(X)\n",
    "        \n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "        \n",
    "    # Validation loop\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    model_lenet5v1.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in val_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            val_acc += acc\n",
    "            \n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37658970-2e82-405f-8699-4ba457d464a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    total_params = 0\n",
    "    for layer_names, param in model.named_parameters():\n",
    "        total_params += torch.count_nonzero(param.data)\n",
    "    return total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7184f902-044e-4cb8-a5b3-4f1a484941b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpruned LeNet-4 model has 1199882 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "orig_params = count_params(model_lenet5v1)\n",
    "print(f\"Unpruned LeNet-5 model has {orig_params} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec4a58de-1685-43b4-b644-91ba3db59438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.name: conv1.weight & param.shape = torch.Size([32, 1, 3, 3])\n",
      "layer.name: conv1.bias & param.shape = torch.Size([32])\n",
      "layer.name: conv2.weight & param.shape = torch.Size([64, 32, 3, 3])\n",
      "layer.name: conv2.bias & param.shape = torch.Size([64])\n",
      "layer.name: fc1.weight & param.shape = torch.Size([128, 9216])\n",
      "layer.name: fc1.bias & param.shape = torch.Size([128])\n",
      "layer.name: fc2.weight & param.shape = torch.Size([10, 128])\n",
      "layer.name: fc2.bias & param.shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer, param in model_lenet5v1.named_parameters():\n",
    "    print(f\"layer.name: {layer} & param.shape = {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e89617fc-d894-43e1-a737-29cecf25b6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([32, 1, 3, 3])\n",
      "conv1.bias torch.Size([32])\n",
      "conv2.weight torch.Size([64, 32, 3, 3])\n",
      "conv2.bias torch.Size([64])\n",
      "fc1.weight torch.Size([128, 9216])\n",
      "fc1.bias torch.Size([128])\n",
      "fc2.weight torch.Size([10, 128])\n",
      "fc2.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for layer_name in model_lenet5v1.state_dict().keys():\n",
    "    print(layer_name, model_lenet5v1.state_dict()[layer_name].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "367d8c12-508c-467f-a544-49ac4bf7821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lenet5v1.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a5380d8-3a4b-4bbc-8328-3a710134c9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sparsity(model):\n",
    "    conv1_sparsity = (torch.sum(model.conv1.weight == 0) / model.conv1.weight.nelement()) * 100\n",
    "    conv2_sparsity = (torch.sum(model.conv2.weight == 0) / model.conv2.weight.nelement()) * 100\n",
    "    fc1_sparsity = (torch.sum(model.fc1.weight == 0) / model.fc1.weight.nelement()) * 100\n",
    "    op_sparsity = (torch.sum(model.fc2.weight == 0) / model.fc2.weight.nelement()) * 100\n",
    "\n",
    "    num = torch.sum(model.conv1.weight == 0) + torch.sum(model.conv2.weight == 0) + torch.sum(model.fc1.weight == 0) + torch.sum(model.fc2.weight == 0)\n",
    "    denom = model.conv1.weight.nelement() + model.conv2.weight.nelement() + model.fc1.weight.nelement() + model.fc2.weight.nelement()\n",
    "\n",
    "    global_sparsity = num/denom * 100\n",
    "\n",
    "    return global_sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbeb79bc-735b-4371-b53a-f91a14ac2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet-5 global sparsity = 0.00%\n"
     ]
    }
   ],
   "source": [
    "print(f\"LeNet-5 global sparsity = {compute_sparsity(model_lenet5v1):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0518e81c-0c47-49be-8719-2e396696fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Iterative Global pruning round = 1\n",
      "LeNet-5 global sparsity = 10.14%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.03098| Train acc:  0.99024| Val loss:  0.04797| Val acc:  0.99069\n",
      "Epoch: 1| Train loss:  0.02270| Train acc:  0.99265| Val loss:  0.05705| Val acc:  0.99003\n",
      "Epoch: 2| Train loss:  0.02143| Train acc:  0.99271| Val loss:  0.05703| Val acc:  0.98953\n",
      "Epoch: 3| Train loss:  0.01961| Train acc:  0.99382| Val loss:  0.05070| Val acc:  0.99069\n",
      "Epoch: 4| Train loss:  0.02046| Train acc:  0.99332| Val loss:  0.06339| Val acc:  0.99019\n",
      "Epoch: 5| Train loss:  0.02047| Train acc:  0.99335| Val loss:  0.06260| Val acc:  0.99019\n",
      "Epoch: 6| Train loss:  0.01653| Train acc:  0.99454| Val loss:  0.05469| Val acc:  0.98986\n",
      "Epoch: 7| Train loss:  0.01741| Train acc:  0.99445| Val loss:  0.05249| Val acc:  0.99019\n",
      "Epoch: 8| Train loss:  0.01651| Train acc:  0.99496| Val loss:  0.05503| Val acc:  0.99086\n",
      "Epoch: 9| Train loss:  0.01754| Train acc:  0.99450| Val loss:  0.05124| Val acc:  0.99202\n",
      "Epoch: 10| Train loss:  0.01643| Train acc:  0.99506| Val loss:  0.06041| Val acc:  0.99003\n",
      "Epoch: 11| Train loss:  0.01582| Train acc:  0.99539| Val loss:  0.06862| Val acc:  0.98870\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 2\n",
      "LeNet-5 global sparsity = 19.52%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.15825| Train acc:  0.89572| Val loss:  0.18527| Val acc:  0.89495\n",
      "Epoch: 1| Train loss:  0.15110| Train acc:  0.89755| Val loss:  0.19545| Val acc:  0.89362\n",
      "Epoch: 2| Train loss:  0.15215| Train acc:  0.89735| Val loss:  0.19311| Val acc:  0.89777\n",
      "Epoch: 3| Train loss:  0.15111| Train acc:  0.89809| Val loss:  0.20810| Val acc:  0.89661\n",
      "Epoch: 4| Train loss:  0.15186| Train acc:  0.89799| Val loss:  0.19114| Val acc:  0.89395\n",
      "Epoch: 5| Train loss:  0.14931| Train acc:  0.89697| Val loss:  0.20115| Val acc:  0.89727\n",
      "Epoch: 6| Train loss:  0.15280| Train acc:  0.89755| Val loss:  0.20121| Val acc:  0.89694\n",
      "Epoch: 7| Train loss:  0.14882| Train acc:  0.89857| Val loss:  0.20268| Val acc:  0.89461\n",
      "Epoch: 8| Train loss:  0.14942| Train acc:  0.89868| Val loss:  0.19459| Val acc:  0.89894\n",
      "Epoch: 9| Train loss:  0.15047| Train acc:  0.89710| Val loss:  0.21256| Val acc:  0.89744\n",
      "Epoch: 10| Train loss:  0.15124| Train acc:  0.89797| Val loss:  0.19540| Val acc:  0.89744\n",
      "Epoch: 11| Train loss:  0.14938| Train acc:  0.89740| Val loss:  0.19146| Val acc:  0.89827\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 3\n",
      "LeNet-5 global sparsity = 27.33%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.33444| Train acc:  0.80817| Val loss:  0.37478| Val acc:  0.80352\n",
      "Epoch: 1| Train loss:  0.33048| Train acc:  0.80848| Val loss:  0.37072| Val acc:  0.80402\n",
      "Epoch: 2| Train loss:  0.32945| Train acc:  0.80806| Val loss:  0.37943| Val acc:  0.80236\n",
      "Epoch: 3| Train loss:  0.32890| Train acc:  0.80878| Val loss:  0.37435| Val acc:  0.80402\n",
      "Epoch: 4| Train loss:  0.32706| Train acc:  0.80776| Val loss:  0.37212| Val acc:  0.80352\n",
      "Epoch: 5| Train loss:  0.32664| Train acc:  0.80900| Val loss:  0.37514| Val acc:  0.80419\n",
      "Epoch: 6| Train loss:  0.32704| Train acc:  0.80933| Val loss:  0.37990| Val acc:  0.80037\n",
      "Epoch: 7| Train loss:  0.32750| Train acc:  0.80852| Val loss:  0.37729| Val acc:  0.80269\n",
      "Epoch: 8| Train loss:  0.32695| Train acc:  0.80957| Val loss:  0.38976| Val acc:  0.80286\n",
      "Epoch: 9| Train loss:  0.32586| Train acc:  0.80885| Val loss:  0.37531| Val acc:  0.80352\n",
      "Epoch: 10| Train loss:  0.32553| Train acc:  0.80876| Val loss:  0.37705| Val acc:  0.80386\n",
      "Epoch: 11| Train loss:  0.32625| Train acc:  0.80985| Val loss:  0.37540| Val acc:  0.80369\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 4\n",
      "LeNet-5 global sparsity = 34.38%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.56463| Train acc:  0.70537| Val loss:  0.57925| Val acc:  0.70778\n",
      "Epoch: 1| Train loss:  0.55265| Train acc:  0.70925| Val loss:  0.59510| Val acc:  0.70761\n",
      "Epoch: 2| Train loss:  0.55104| Train acc:  0.70894| Val loss:  0.59032| Val acc:  0.70695\n",
      "Epoch: 3| Train loss:  0.55068| Train acc:  0.70960| Val loss:  0.59223| Val acc:  0.70711\n",
      "Epoch: 4| Train loss:  0.54887| Train acc:  0.70979| Val loss:  0.59894| Val acc:  0.70828\n",
      "Epoch: 5| Train loss:  0.54820| Train acc:  0.70966| Val loss:  0.58740| Val acc:  0.70628\n",
      "Epoch: 6| Train loss:  0.54998| Train acc:  0.70923| Val loss:  0.58740| Val acc:  0.70844\n",
      "Epoch: 7| Train loss:  0.54719| Train acc:  0.70962| Val loss:  0.59644| Val acc:  0.70745\n",
      "Epoch: 8| Train loss:  0.54705| Train acc:  0.70981| Val loss:  0.60289| Val acc:  0.70728\n",
      "Epoch: 9| Train loss:  0.54673| Train acc:  0.71105| Val loss:  0.59458| Val acc:  0.70562\n",
      "Epoch: 10| Train loss:  0.54734| Train acc:  0.70835| Val loss:  0.58947| Val acc:  0.70678\n",
      "Epoch: 11| Train loss:  0.54675| Train acc:  0.70951| Val loss:  0.60344| Val acc:  0.70878\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 5\n",
      "LeNet-5 global sparsity = 40.64%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80410| Train acc:  0.61230| Val loss:  0.82019| Val acc:  0.60672\n",
      "Epoch: 1| Train loss:  0.79347| Train acc:  0.61019| Val loss:  0.82451| Val acc:  0.60721\n",
      "Epoch: 2| Train loss:  0.79248| Train acc:  0.61280| Val loss:  0.82362| Val acc:  0.60256\n",
      "Epoch: 3| Train loss:  0.78976| Train acc:  0.61306| Val loss:  0.83626| Val acc:  0.60588\n",
      "Epoch: 4| Train loss:  0.79037| Train acc:  0.61361| Val loss:  0.83031| Val acc:  0.60705\n",
      "Epoch: 5| Train loss:  0.79177| Train acc:  0.61141| Val loss:  0.83166| Val acc:  0.60605\n",
      "Epoch: 6| Train loss:  0.78948| Train acc:  0.61461| Val loss:  0.82882| Val acc:  0.60655\n",
      "Epoch: 7| Train loss:  0.78936| Train acc:  0.61215| Val loss:  0.82960| Val acc:  0.60721\n",
      "Epoch: 8| Train loss:  0.78818| Train acc:  0.61213| Val loss:  0.83248| Val acc:  0.60721\n",
      "Epoch: 9| Train loss:  0.78728| Train acc:  0.61356| Val loss:  0.82840| Val acc:  0.60705\n",
      "Epoch: 10| Train loss:  0.78829| Train acc:  0.61232| Val loss:  0.83453| Val acc:  0.60306\n",
      "Epoch: 11| Train loss:  0.78738| Train acc:  0.61184| Val loss:  0.82837| Val acc:  0.60721\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 6\n",
      "LeNet-5 global sparsity = 46.88%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.79729| Train acc:  0.61184| Val loss:  0.82468| Val acc:  0.60655\n",
      "Epoch: 1| Train loss:  0.79335| Train acc:  0.61189| Val loss:  0.81959| Val acc:  0.60738\n",
      "Epoch: 2| Train loss:  0.79075| Train acc:  0.61460| Val loss:  0.82228| Val acc:  0.60738\n",
      "Epoch: 3| Train loss:  0.79150| Train acc:  0.61434| Val loss:  0.82656| Val acc:  0.60705\n",
      "Epoch: 4| Train loss:  0.78999| Train acc:  0.61439| Val loss:  0.82685| Val acc:  0.60672\n",
      "Epoch: 5| Train loss:  0.79092| Train acc:  0.61386| Val loss:  0.83372| Val acc:  0.60771\n",
      "Epoch: 6| Train loss:  0.79035| Train acc:  0.61389| Val loss:  0.83555| Val acc:  0.60638\n",
      "Epoch: 7| Train loss:  0.79074| Train acc:  0.61313| Val loss:  0.81990| Val acc:  0.60805\n",
      "Epoch: 8| Train loss:  0.78989| Train acc:  0.61289| Val loss:  0.82528| Val acc:  0.60638\n",
      "Epoch: 9| Train loss:  0.79061| Train acc:  0.61310| Val loss:  0.82508| Val acc:  0.60638\n",
      "Epoch: 10| Train loss:  0.78935| Train acc:  0.61258| Val loss:  0.83023| Val acc:  0.60838\n",
      "Epoch: 11| Train loss:  0.79115| Train acc:  0.61199| Val loss:  0.83983| Val acc:  0.61104\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 7\n",
      "LeNet-5 global sparsity = 52.33%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80145| Train acc:  0.61110| Val loss:  0.82512| Val acc:  0.60738\n",
      "Epoch: 1| Train loss:  0.79552| Train acc:  0.61243| Val loss:  0.82217| Val acc:  0.60655\n",
      "Epoch: 2| Train loss:  0.79237| Train acc:  0.61200| Val loss:  0.82423| Val acc:  0.60788\n",
      "Epoch: 3| Train loss:  0.79345| Train acc:  0.61269| Val loss:  0.81842| Val acc:  0.60755\n",
      "Epoch: 4| Train loss:  0.79291| Train acc:  0.61187| Val loss:  0.82949| Val acc:  0.60588\n",
      "Epoch: 5| Train loss:  0.79387| Train acc:  0.61299| Val loss:  0.83008| Val acc:  0.60771\n",
      "Epoch: 6| Train loss:  0.79163| Train acc:  0.61437| Val loss:  0.82860| Val acc:  0.60755\n",
      "Epoch: 7| Train loss:  0.79210| Train acc:  0.61200| Val loss:  0.83176| Val acc:  0.60688\n",
      "Epoch: 8| Train loss:  0.79150| Train acc:  0.61217| Val loss:  0.84173| Val acc:  0.60289\n",
      "Epoch: 9| Train loss:  0.79036| Train acc:  0.61215| Val loss:  0.82454| Val acc:  0.60755\n",
      "Epoch: 10| Train loss:  0.79194| Train acc:  0.61261| Val loss:  0.82921| Val acc:  0.60738\n",
      "Epoch: 11| Train loss:  0.79092| Train acc:  0.61189| Val loss:  0.84260| Val acc:  0.60688\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 8\n",
      "LeNet-5 global sparsity = 57.01%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.81617| Train acc:  0.60938| Val loss:  0.82532| Val acc:  0.60605\n",
      "Epoch: 1| Train loss:  0.80251| Train acc:  0.61089| Val loss:  0.82775| Val acc:  0.60688\n",
      "Epoch: 2| Train loss:  0.79879| Train acc:  0.61026| Val loss:  0.83061| Val acc:  0.60688\n",
      "Epoch: 3| Train loss:  0.79992| Train acc:  0.60938| Val loss:  0.82937| Val acc:  0.60688\n",
      "Epoch: 4| Train loss:  0.79693| Train acc:  0.61112| Val loss:  0.82589| Val acc:  0.60771\n",
      "Epoch: 5| Train loss:  0.79613| Train acc:  0.61099| Val loss:  0.82949| Val acc:  0.60588\n",
      "Epoch: 6| Train loss:  0.79569| Train acc:  0.61050| Val loss:  0.82601| Val acc:  0.60688\n",
      "Epoch: 7| Train loss:  0.79502| Train acc:  0.61132| Val loss:  0.82361| Val acc:  0.60755\n",
      "Epoch: 8| Train loss:  0.79442| Train acc:  0.61287| Val loss:  0.82911| Val acc:  0.60688\n",
      "Epoch: 9| Train loss:  0.79605| Train acc:  0.61052| Val loss:  0.82755| Val acc:  0.60705\n",
      "Epoch: 10| Train loss:  0.79515| Train acc:  0.61232| Val loss:  0.82131| Val acc:  0.60688\n",
      "Epoch: 11| Train loss:  0.79508| Train acc:  0.61289| Val loss:  0.82407| Val acc:  0.60688\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 9\n",
      "LeNet-5 global sparsity = 61.69%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80443| Train acc:  0.60930| Val loss:  0.82833| Val acc:  0.60605\n",
      "Epoch: 1| Train loss:  0.80086| Train acc:  0.61169| Val loss:  0.82198| Val acc:  0.61037\n",
      "Epoch: 2| Train loss:  0.80034| Train acc:  0.61019| Val loss:  0.83519| Val acc:  0.60605\n",
      "Epoch: 3| Train loss:  0.80013| Train acc:  0.61030| Val loss:  0.82654| Val acc:  0.60705\n",
      "Epoch: 4| Train loss:  0.80075| Train acc:  0.60865| Val loss:  0.82512| Val acc:  0.60738\n",
      "Epoch: 5| Train loss:  0.79967| Train acc:  0.61050| Val loss:  0.81858| Val acc:  0.60705\n",
      "Epoch: 6| Train loss:  0.79720| Train acc:  0.61073| Val loss:  0.82282| Val acc:  0.60605\n",
      "Epoch: 7| Train loss:  0.79876| Train acc:  0.61100| Val loss:  0.82657| Val acc:  0.60572\n",
      "Epoch: 8| Train loss:  0.79586| Train acc:  0.61104| Val loss:  0.82053| Val acc:  0.60721\n",
      "Epoch: 9| Train loss:  0.79657| Train acc:  0.61121| Val loss:  0.82377| Val acc:  0.60721\n",
      "Epoch: 10| Train loss:  0.79599| Train acc:  0.61147| Val loss:  0.82428| Val acc:  0.60672\n",
      "Epoch: 11| Train loss:  0.79740| Train acc:  0.60997| Val loss:  0.82475| Val acc:  0.60705\n",
      "\n",
      "\n",
      "Iterative Global pruning round = 10\n",
      "LeNet-5 global sparsity = 65.58%\n",
      "\n",
      "Fine-tuning pruned model to recover model's performance\n",
      "\n",
      "Epoch: 0| Train loss:  0.80703| Train acc:  0.60928| Val loss:  0.82964| Val acc:  0.60622\n",
      "Epoch: 1| Train loss:  0.80346| Train acc:  0.60878| Val loss:  0.81963| Val acc:  0.60655\n",
      "Epoch: 2| Train loss:  0.80386| Train acc:  0.61062| Val loss:  0.82311| Val acc:  0.60655\n",
      "Epoch: 3| Train loss:  0.80013| Train acc:  0.61025| Val loss:  0.82486| Val acc:  0.60339\n",
      "Epoch: 4| Train loss:  0.80180| Train acc:  0.60982| Val loss:  0.82418| Val acc:  0.60622\n",
      "Epoch: 5| Train loss:  0.80222| Train acc:  0.61054| Val loss:  0.83076| Val acc:  0.60622\n",
      "Epoch: 6| Train loss:  0.80305| Train acc:  0.61045| Val loss:  0.82436| Val acc:  0.60672\n",
      "Epoch: 7| Train loss:  0.80163| Train acc:  0.60965| Val loss:  0.82035| Val acc:  0.60339\n",
      "Epoch: 8| Train loss:  0.80094| Train acc:  0.60899| Val loss:  0.82682| Val acc:  0.60638\n",
      "Epoch: 9| Train loss:  0.80056| Train acc:  0.61128| Val loss:  0.82722| Val acc:  0.60755\n",
      "Epoch: 10| Train loss:  0.79786| Train acc:  0.61169| Val loss:  0.82201| Val acc:  0.60356\n",
      "Epoch: 11| Train loss:  0.80170| Train acc:  0.61080| Val loss:  0.82716| Val acc:  0.60638\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "for iter_prune_round in range(10):\n",
    "    print(f\"\\n\\nIterative Global pruning round = {iter_prune_round + 1}\")\n",
    "    \n",
    "    # Prune layer-wise in a structured manner-\n",
    "    prune.ln_structured(model_lenet5v1.conv1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.conv2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.fc1, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "    prune.ln_structured(model_lenet5v1.fc2, name = \"weight\", amount = 0.1, n = 2, dim = 0)\n",
    "\n",
    "    # Print current global sparsity level-\n",
    "    print(f\"LeNet-5 global sparsity = {compute_sparsity(model_lenet5v1):.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Fine-training loop-\n",
    "    print(\"\\nFine-tuning pruned model to recover model's performance\\n\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for X, y in train_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            model_lenet5v1.train()\n",
    "            \n",
    "            y_pred = model_lenet5v1(X)\n",
    "            \n",
    "            loss = loss_fn(y_pred, y)\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            acc = accuracy(y_pred, y)\n",
    "            train_acc += acc\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "            \n",
    "        # Validation loop\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        model_lenet5v1.eval()\n",
    "        with torch.inference_mode():\n",
    "            for X, y in val_dataloader:\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                \n",
    "                y_pred = model_lenet5v1(X)\n",
    "                \n",
    "                loss = loss_fn(y_pred, y)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                acc = accuracy(y_pred, y)\n",
    "                val_acc += acc\n",
    "                \n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc /= len(val_dataloader)\n",
    "        \n",
    "        print(f\"Epoch: {epoch}| Train loss: {train_loss: .5f}| Train acc: {train_acc: .5f}| Val loss: {val_loss: .5f}| Val acc: {val_acc: .5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691c0d6-54b7-42b3-b715-5be196b1695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
