{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e84dba5-ae3d-4f14-b7ba-19273f445021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import kornia\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42ccc03-1ff6-4faa-87a1-64bdd419db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "}\n",
    "\n",
    "model_path = './pretrained_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9757bb-42b5-4f57-b646-8e4d7efa71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f194b97a-444a-499c-be49-6baf1f4d2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1x1(in_planes, out_planes, stride =1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride = stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6149d472-0539-4d2c-9841-53646a0ea152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_denoising_block(torch.nn.Module):\n",
    "    ''' Simple filters as denoising block'''\n",
    "    def __init__(self, in_planes, ksize, filter_type): \n",
    "        super().__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.ksize = ksize\n",
    "        self.filter_type = filter_type \n",
    "        self.conv = nn.Conv2d(in_channels=in_planes, out_channels=in_planes, kernel_size=1, stride=1, padding=0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.filter_type == 'Median':\n",
    "            x_denoised = kornia.filters.median_blur(x, (self.ksize, self.ksize))\n",
    "        elif self.filter_type == 'Mean':\n",
    "            x_denoised = kornia.filters.box_blur(x, (self.ksize, self.ksize))\n",
    "        elif self.filter_type == 'Gaussian':\n",
    "            x_denoised = kornia.filters.gaussian_blur2d(x, (self.ksize, self.ksize), (0.3 * ((x.shape[3] - 1) * 0.5 - 1) + 0.8, 0.3 * ((x.shape[2] - 1) * 0.5 - 1) + 0.8))\n",
    "        new_x = x + self.conv(x_denoised)\n",
    "        return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8369a93d-6167-43cd-9337-cab0d80913f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class non_local_denoising_block(torch.nn.Module): \n",
    "    def __init__(self, in_channels, inter_channels = None, mode = 'embedded', bn_layer = True):\n",
    "        \n",
    "        \"\"\" in_channels: original channel size (1024 in the paper) #depends on the test dataset\n",
    "            inter_channels: channel size inside the block if not specifed reduced to half (512 in the paper)\n",
    "            mode: supports Gaussian, Embedded Gaussian, Dot Product, and Concatenation\n",
    "            dimension: can be 1 (temporal), 2 (spatial), 3 (spatiotemporal)\n",
    "            bn_layer: whether to add batch norm\n",
    "            \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        self.mode = mode \n",
    "        self.inter_channels = inter_channels\n",
    "        \n",
    "         # the channel size is reduced to half inside the block\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "        \n",
    "        \n",
    "        #dimension=2 for images\n",
    "        conv_nd = nn.Conv2d \n",
    "        max_pool_layer = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        bn = nn.BatchNorm2d\n",
    "        \n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1)\n",
    "        \n",
    "        # add BatchNorm layer after the last conv layer\n",
    "        if bn_layer:\n",
    "            self.W_z = nn.Sequential(\n",
    "                    conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1),\n",
    "                    bn(self.in_channels)\n",
    "                )\n",
    "            # from section 4.1 of the paper, initializing params of BN ensures that the initial state of non-local block is identity mapping\n",
    "            nn.init.constant_(self.W_z[1].weight, 0)\n",
    "            nn.init.constant_(self.W_z[1].bias, 0)\n",
    "        else:\n",
    "            self.W_z = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels, kernel_size=1)\n",
    "            \n",
    "            # from section 3.3 of the paper by initializing Wz to 0, this block can be inserted to any existing architecture\n",
    "            nn.init.constant_(self.W_z.weight, 0)\n",
    "            nn.init.constant_(self.W_z.bias, 0)\n",
    "            \n",
    "            \n",
    "        # define theta and phi for all operations except gaussian\n",
    "        if self.mode == \"embedded\" or self.mode == \"dot\" or self.mode == \"concatenate\":\n",
    "            self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1)\n",
    "            self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels, kernel_size=1)\n",
    "        \n",
    "        if self.mode == \"concatenate\":\n",
    "            self.W_f = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels=self.inter_channels * 2, out_channels=1, kernel_size=1),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "        \n",
    "        if self.mode == \"nonlocal_gaussian\":\n",
    "            theta_x = x.view(batch_size, self.in_channels, -1)\n",
    "            phi_x = x.view(batch_size, self.in_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "\n",
    "        elif self.mode == \"embedded\" or self.mode == \"dot\":\n",
    "            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "            phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "\n",
    "        elif self.mode == \"concatenate\":\n",
    "            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n",
    "            phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n",
    "            \n",
    "            h = theta_x.size(2)\n",
    "            w = phi_x.size(3)\n",
    "            theta_x = theta_x.repeat(1, 1, 1, w)\n",
    "            phi_x = phi_x.repeat(1, 1, h, 1)\n",
    "            \n",
    "            concat = torch.cat([theta_x, phi_x], dim=1)\n",
    "            f = self.W_f(concat)\n",
    "            f = f.view(f.size(0), f.size(2), f.size(3))\n",
    "        \n",
    "        if self.mode == \"nonlocal_gaussian\" or self.mode == \"embedded\":\n",
    "            f_div_C = F.softmax(f, dim=-1)\n",
    "        elif self.mode == \"dot\" or self.mode == \"concatenate\":\n",
    "            N = f.size(-1) # number of position in x\n",
    "            f_div_C = f / N\n",
    "        \n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        \n",
    "        # contiguous here just allocates contiguous chunk of memory\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        \n",
    "        W_y = self.W_z(y)\n",
    "        # residual connection\n",
    "        z = W_y + x\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a18e22-b9eb-49ad-859e-b9a125e5ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    # class attribute\n",
    "    expansion = 1\n",
    "    num_layers = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # only conv with possibly not 1 stride\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        # if stride is not 1 then self.downsample cannot be None\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # the residual connection\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def block_conv_info(self):\n",
    "        block_kernel_sizes = [3, 3]\n",
    "        block_strides = [self.stride, 1]\n",
    "        block_paddings = [1, 1]\n",
    "\n",
    "        return block_kernel_sizes, block_strides, block_paddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2459b9d1-0f32-4d3f-adcf-e8bdb2dbc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # class attribute\n",
    "    expansion = 4\n",
    "    num_layers = 3\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        # only conv with possibly not 1 stride\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # if stride is not 1 then self.downsample cannot be None\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def block_conv_info(self):\n",
    "        block_kernel_sizes = [1, 3, 1]\n",
    "        block_strides = [1, self.stride, 1]\n",
    "        block_paddings = [0, 1, 0]\n",
    "\n",
    "        return block_kernel_sizes, block_strides, block_paddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42bb576e-d2f1-42e1-b123-cc93ddecc35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filter_block(denoise, in_channels, ksize):\n",
    "    # add a denoising layer\n",
    "    if denoise is None or denoise == 'None':\n",
    "        return None\n",
    "    if denoise == \"Mean\" or denoise == \"Median\" or denoise == \"Gaussian\":\n",
    "        return filter_denoising_block(in_planes = in_channels, ksize = ksize, filter_type=denoise)\n",
    "    \n",
    "    elif denoise == \"concatenate\" or denoise == \"dot\" or denoise == \"embedded\" or denoise == \"nonlocal_gaussian\" :\n",
    "       return non_local_denoising_block(in_channels = in_channels,  mode=denoise) \n",
    "    else:\n",
    "        raise ValueError('Unknown filter type!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a36ea59b-2a9f-4b31-845c-2ea19f265be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_features(nn.Module):\n",
    "    '''\n",
    "    the convolutional layers of ResNet\n",
    "    the average pooling and final fully convolutional layer is removed\n",
    "    '''\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, filter=None, filter_layer=None):\n",
    "        super(ResNet_features, self).__init__()\n",
    "        self.inplanes = 64\n",
    "\n",
    "        # the first convolutional layer before the structured sequence of blocks\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.global_pool = nn.AvgPool2d(kernel_size=7)\n",
    "\n",
    "        # comes from the first conv and the following max pool\n",
    "        self.kernel_sizes = [7, 3]\n",
    "        self.strides = [2, 2]\n",
    "        self.paddings = [3, 1]\n",
    "\n",
    "        # the following layers, each layer is a sequence of blocks\n",
    "        self.block = block\n",
    "        self.layers = layers\n",
    "\n",
    "\n",
    "        self.layer1 = self._make_layer(block=block, planes=64, num_blocks=self.layers[0])\n",
    "        self.dn_layer1 = None\n",
    "        if filter_layer in [1,9]:\n",
    "            self.dn_layer1 = get_filter_block(filter, 64, 3)\n",
    "\n",
    "\n",
    "        self.layer2 = self._make_layer(block=block, planes=128, num_blocks=self.layers[1], stride=2)\n",
    "        self.dn_layer2 = None\n",
    "        if filter_layer in [2,9]:\n",
    "            self.dn_layer2 = get_filter_block(filter, 128, 3)\n",
    "\n",
    "\n",
    "        self.layer3 = self._make_layer(block=block, planes=256, num_blocks=self.layers[2], stride=2)\n",
    "        self.dn_layer3 = None\n",
    "        if filter_layer in [3,9]:\n",
    "            self.dn_layer3 = get_filter_block(filter, 256, 3)\n",
    "\n",
    "\n",
    "        self.layer4 = self._make_layer(block=block, planes=512, num_blocks=self.layers[3], stride=2)\n",
    "        self.dn_layer4 = None\n",
    "        if filter_layer in [4,9]:\n",
    "            self.dn_layer4 = get_filter_block(filter, 512, 3)\n",
    "\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "\n",
    "        # initialize the parameters\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        # only the first block has downsample that is possibly not None\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        # keep track of every block's conv size, stride size, and padding size\n",
    "        for each_block in layers:\n",
    "            block_kernel_sizes, block_strides, block_paddings = each_block.block_conv_info()\n",
    "            self.kernel_sizes.extend(block_kernel_sizes)\n",
    "            self.strides.extend(block_strides)\n",
    "            self.paddings.extend(block_paddings)\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        if self.dn_layer1 is not None:\n",
    "            x = self.dn_layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        if self.dn_layer2 is not None:\n",
    "            x = self.dn_layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        if self.dn_layer3 is not None:\n",
    "            x = self.dn_layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        if self.dn_layer4 is not None:\n",
    "            x = self.dn_layer4(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.reshape(-1, 512)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_all(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        all_feas = []\n",
    "        for i in range(4):\n",
    "            x = getattr(self, 'layer{}'.format(i + 1))(x)\n",
    "            all_feas.append(x)\n",
    "\n",
    "        return x, all_feas\n",
    "\n",
    "    def forward_shallow(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        for i in range(1):\n",
    "            x = getattr(self, 'layer{}'.format(i + 1))(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward_shallow_to_deep(self, shallow_feas):\n",
    "        x = shallow_feas\n",
    "        for i in range(1, 4):\n",
    "            x = getattr(self, 'layer{}'.format(i + 1))(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def conv_info(self):\n",
    "        return self.kernel_sizes, self.strides, self.paddings\n",
    "\n",
    "    def num_layers(self):\n",
    "        '''\n",
    "        the number of conv layers in the network, not counting the number\n",
    "        of bypass layers\n",
    "        '''\n",
    "\n",
    "        return (self.block.num_layers * self.layers[0]\n",
    "              + self.block.num_layers * self.layers[1]\n",
    "              + self.block.num_layers * self.layers[2]\n",
    "              + self.block.num_layers * self.layers[3]\n",
    "              + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "946583f3-9a0f-4e37-87ad-15e0bbf8c745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18_features(pretrained=False, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on DET\n",
    "    \"\"\"\n",
    "    model = ResNet_features(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        my_dict = model_zoo.load_url(model_urls['resnet18'], model_dir=model_dir)\n",
    "        my_dict.pop('fc.weight')\n",
    "        my_dict.pop('fc.bias')\n",
    "        model.load_state_dict(my_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b239a-a5af-41c9-ab7c-89e5333a0bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
